[{"title":"Flink-流处理分区策略","date":"2022-03-03T07:04:00.000Z","url":"/2022/03/03/Flink-%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5/","tags":[["Flink","/tags/Flink/"],["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"]],"categories":[[" ",""]],"content":" [toc] Flink 针对流处理不同并发度和不同场景定义了多种分区策略（Partitioner），这些分区器有一个统一的接口 ChannelSelector，还有一个统一的基类 StreamPartitoner，下图是相关类的类图。 现在对这些分区器进行简单的分析。 ChannelSelector首先看 ChannelSelector 接口 由上面代码可知，最关键的方法就是 selectChannel 方法。 StreamPartitioner然后再看看这些方法的基类 StreamPartitioner GlobalPartitionerDataStream =&gt; DataStream 使用方式，作用于 DataStream =&gt; DataStream 直接 .global ForwardPartitionerDataStream =&gt; DataStream 这么一看好像 GlobalPartitioner 和 ForwardPartitioner 没什么不一样，但存在即合理，因为ForwardPartitioner 有个重要的前提条件就是上下游的并发度必须一样，如果不一样是会抛异常的。由示意图也能看出来，GlobalPartitioner 是将上游所有算子的数据发给下游一个算子，而 ForwardPartitioner 则是上游的算子和下游的算子是 1:1 对应的。 下面是 ForwardPartitioner 设置条件及抛出异常的相关代码 使用方式，作用于 DataStream =&gt; DataStream 直接 .forward RescalePartitionerDataStream =&gt; DataStream Rescale 可以翻译为缩放，顾名思义，RescalePartitioner 可以通过缩放处理上下游算子并发度不一样的情况。 例如，上游并发度为 2 分别为 A、B，下游并发度为 4 分别为 1、2、3、4，RescalePartitioner 会将 A 的数据发给 1、2，将 B 的数据发给 3、4；再比如上游并发度为 4 分别为 A、B、C、D，下游并非度为 2 分别为 1、2，RescalePartitioner 会将 A、B 的数据发给 1，将 C、D 的数据发给 2。 使用方式，作用于 DataStream =&gt; DataStream 直接 .rescale RebalancePartitionerDataStream =&gt; DataStream RebalancePartitioner 会循环发送给下游每一个实例 使用方式，作用于 DataStream =&gt; DataStream 直接 .rebalance ShufflePartitionerDataStream =&gt; DataStream 使用方式，作用于 DataStream =&gt; DataStream 直接 .shuffle BroadcastPartitionerDataStream =&gt; DataStream BroadcastPartitioner 会将上游实例的数据发给下游每一个实例，相当于广播 使用方式，作用于 DataStream =&gt; DataStream 直接 .broadcast KeyGroupStreamPartitionerKeyGroupStreamPartitioner 是根据 key 取 hash 分组选择下游 channel 使用方式，直接 .keyBy，并传入 key selector CustomPartitionerWrapperCustomPartitionerWrapper 是用户自定义的分区器 例如 使用方式，partitonCustom 传入自定义的分区器 参考资料： Flink的八种分区策略源码解读 本文所有图片均来自Flink的八种分区策略源码解读，侵删"},{"title":"Flink-1.14 源码分析--StreamGraph 生成过程","date":"2022-01-22T08:35:09.000Z","url":"/2022/01/22/Flink%20StreamGraph%20%E7%94%9F%E6%88%90%E8%BF%87%E7%A8%8B/","tags":[["Flink","/tags/Flink/"],["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["源码分析","/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"]],"categories":[[" ",""]],"content":" 友情提示，本文可能会引起不适，请倒好一杯咖啡或泡好一杯茶，打开 Flink 源码跟我一起追源码 在 Flink StreamGraph 生成过程前置篇 中介绍了很多关于 Flink StreamGraph 的概念，这篇将介绍 Flink StreamGraph 是怎么生成的。还是以官方 org.apache.flink.streaming.examples.wordcount.WordCount 为例。 通过 env.getExecutionPlan() 获取执行计划并在 flink visualizer 中生成的执行计划为下图所示 下面来看看这个图是怎么生成的，我么从 env.execute() 往里面跟，能够看到 getStreamGraph() 方法（中间有些无关的方法调用省略了） 从上面代码可看到，先获取一个 StreamGraphGenerator 再调用其 generate() 方法生成 StreamGraph，在查看 generate() 方法前，先看下给 getStreamGraphGenerator 方法的参数 transformations。transformations 是 StreamExecutionEnvironment 中的一个属性，用于存放 Transformation 列表，通过 addOperator() 方法可以为其添加元素。 在上篇文章中我们提到了 Transformation 是 DataStream API 到 Flink 内核的一个过渡，那么我们使用的 DataStream API 最终都可以转换成 Transformation，我们以例子中的调用链为例 首先我们看下 fromElements() 方法，最终会调用到 StreamExecutionEnvironment#addSource 方法并创建一个 DataStreamSource 对象返回，创建 DataStreamSource 时，创建了一个 LegacySourceTransformation，中间方法调用步骤省略了，直接看下 DataStreamSource 的构造方法。 先不管 fromElements() 创建的 LegacySourceTransformation 是怎么与后面的关联起来的，继续往后看 flatMap，经过 flatMap -&gt; transform 最终来到 doTransform。 流程很简单，先 check 一下输出类型，然后创建一个 OneInputTransformation，然后讲 OneInputTransformation 封装到一个 DataStrean（SingleOutputStreamOperator 是 DataStream 的子类）用于后续返回，再将创建的 OneInputTransformation 添加到 StreamExecutionEnvironment 的 transformations 中。 ok 继续看 keyBy()，keyBy 会创建一个 KeyedStream 返回 keyBy() 创建 KeyedStream 时会创建一个 PartitionTransformation 然后就将 KeyedStream 返回了，我们也先不管它是怎么和前后 Transformation 关联起来的，继续往后看 sum() ，sum() 通过 sum() -&gt; aggregate() 最终来到 reduce()。 代码很简单，创建一个 Transformation 添加到 StreamExecutionEnvrionment 的 transformations 中，然后创建一个 DataStream 返回。最后看看 print()，print() 会调用 addSink() 先创建一个 StreamOperator（StreamSink 是 StreamOperator 的实现）然后创建一个 DataStreamSink 对象用于返回，DataStreamSink 可以理解为跟 DataStreamSource 类似的，只不过 DataStreamSink 没有下游了。在 DataStreamSink 中创建了一个 LegacySinkTransformation。 ok 到目前为止每个算子都创建了一个 Transformation，其中除了 LegacySourceTransformation 没有上游、LegacySinkTransformation 没有下游外，其余的 Transformation 都可以根据 input 属性串起来，形成了一条由数据源到数据汇中间连接多个 Transformation 的链。目前 StreamExecutionEnvrionment 中 transformations 仅仅保存着 flatMap 对应的 OneInputTransformation 和 sum 对应的 ReduceTransformation。让我们回到一开始的 getStreamGraph() 中，获取一个 StreamGraphGenerator 并调用其 generate() 方法 显然上述代码的重点就在 transform() 方法里 transform() 方法主要是设置 transformation 的最大并发度，获取 TransformationTranslator 然后调用 translate，Transformation 与 TransformationTranslator 的对应关系存放在 translatorMap 一个 Map 中，然后将这个 Transformation 添加到已经遍历的 Map 中。 translate() 方法首先会通过递归的方式调用 transfom() 方法，一直回溯到 LegacySourceTransformation，除了数据源 Transformation，其它的 Transformation 都有输入 Transformation，之前留下了一个疑问就是 LegacySourceTransformation 是怎么和后面的 OneInputTransformation 关联起来的吗？就是在这里，OneInputTransformation 的 input 就是 LegacySourceTransformation，所以会先处理上游的 Transformation。 我们就已 OneInputTransformation 对应的 OneInputTransformationTranslator 为例，看下 translateForStreaming 中都干了些什么。是一个接口由 org.apache.flink.streaming.api.graph.SimpleTransformationTranslator 实现，并在方法中调用 translateForStreamingInternal，而在 org.apache.flink.streaming.runtime.translators.OneInputTransformationTranslator#translateForStreamingInternal 中又调用父类 AbstractOneInputTransformationTranslator 的 translateInternal() 方法，这里有点乱，跟着源码的调用链走一遍就可以了，Flink 只是将功能相同的方法抽象到了父类中而已，我们这里只需要知道 translateForStreaming() -&gt; translateForStreamingInternal() -&gt; translateInternal() 这个调用链就行了。那么详细看下 translateInternal。 代码逻辑很简单，流程如下 将当前 Transformation 封装为 StreamGraph 的边 设置当前 StreamGraph 节点的并发度 将当前节点与之前的节点相连，也就是添加边 StreamEdge 返回当前 Transformation id 重点方法在 addOperator() 和 addEdge()，我们一个一个来看 上面代码也很简单，就是将 Transformation 封装成 StreamNode 并保存到 streamNodes Map 中。 接下来看下 addEdge() 方法，addEdge() 会调用 addEdgeInternal()，这里就直接放 addEdgeInternal() 的代码了 addEdgeInternal() 是个递归方法，主要有三个分支，旁路输出边、虚拟边和普通边，其中旁路输出和虚拟边逻辑都差不多，获取上下游节点，其中虚拟边自定义了 Partitioner 和 ExchangeMode，然后递归调用 addEdgeInternal()。重点看最后一个分支，也就是 else 分支，大概流程为： 获取 Partition，如果上下有并发度一直则使用 ForwardPartitioner，否则使用 RebalancePartitioner 封装一个 StreamEdge 为上游添加输出 StreamEdge 为下游添加输入 StreamEdge 关于 Partitioner 后面会单独写一篇细聊，好了，挖坑完毕。继续看，经过 addEdge() 方法调用后，我们就得到了 StreamNode -&gt; StreamEdge -&gt; StreamNode -&gt; … 的一条链了。 等等，之前 keyBy() 创建 PartitionTransformation 好像没有关联在里面？对，现在填下之前留下的坑，在遍历 PartitionTransformation 调用 translate() 方法时会传入 PartitionTransformationTranslator（什么，明明 PartitionTransformation 没有在 transformations 这个 list 中为什么会遍历它？因为 PartitionTransformation 是它下游也就是 sum 创建 ReduceTransform 的 input，忘了会递归调用上游 Transformation 了？在遍历到 ReduceTransform 时会先递归调用其上游，也就是 PartitionTransformation），并在 translate 中调用其 translateForStreaming() 方法，好了 translateForStreaming 又是 translateForStreaming() -&gt; translateForStreamingInternal() -&gt; translateInternal() 这个调用链，所以我们直接看 PartitionTransformationTranslator 的 translateInternal() 方法 PartitionTransformationTranslator 的 translateInternal() 方法也很简单，将该 Transformation 的所有 input 都调用 addVirtualPartitionNode 添加到 virtualPartitionNodes 中，virtualPartitionNodes 这个变量是不是有点眼熟，没错，就是 addEdgeInternal 中第二个分支用到了，其 value 是一个三元组，分别保存了 inputTransformationId、partitioner、exchangeMode。 真相大白了，我们来捋一捋，按照遍历顺序，我们会先遍历 PartitionTransformation，在遍历它时会将其 inputTransformationId、partitioner、exchangeMode 保存到 virtualPartitionNodes 中，之后遍历 ReduceTransformation 也就是 sum 算子，生成一个 StreamNode，会为 StreamNode 添加边，添加边时就会取出 virtualPartitionNodes 中保存的 partitioner、exchangeMode，并与 PartitionTransformation 上游相连（第二个分支会递归调用 addEdgeInternal() 方法）此时这条边的 partitioner、exchangeMode 已经设置为 PartitionTransformation 中设置的值。 所以 PartitionTransformation 也会被加入到 StreamGraph 的链中，只不过不是以 StreamNode 的形式存在，而是附着在 StreamEdge 上。 至此，一条有多个 StreamNode 和 StreamEdge 相连的组合而成的 StreamGraph 就生成了，让我们从头来捋一遍，在我们编写代码调用 Stream API，最终调用 execute 后，会将我们使用的 Stream API 算子封装成一个 Transformation 并添加到 StreamExecutionEnvrionment 的 transformations list 中（虚拟节点不会），然后在递归遍历这个 transformations，从最开始的 LegacySourceTransformation 开始创建一个个的 StreamNode，并创建 StreamEdge 连接上游，此时如果上游是一个虚拟节点则将此虚拟节点附着在 StreamEdge 上，并于虚拟节点的上游相连，最终遍历完所有的 Transformation 后 StreamGraph 也生成了。 总结： 其实生成 StreamGraph 的过程并不复杂，只是整个过程涉及的方法、类比较多，只要跟着流程一步一步的多追几遍源码还是可以理解的，或者通过本地 debug 的方式进行跟踪一步一步的看代码执行调用流程。"},{"title":"Flink-1.14 源码分析--StreamGraph 生成过程前置篇","date":"2022-01-11T15:52:09.000Z","url":"/2022/01/11/Flink%20StreamGraph%20%E7%94%9F%E6%88%90%E8%BF%87%E7%A8%8B%E5%89%8D%E7%BD%AE%E7%AF%87/","tags":[["Flink","/tags/Flink/"],["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["源码分析","/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"]],"categories":[[" ",""]],"content":"在 Flink 任务提交流程中 提到在 Flink Client 端会运行我们编写代码中的 main() 方法，然后生成 JobGraph，再将 JobGraph 提交到 Flink Cluster。 说起 Graph，自然而然就想到了 Spark 的 DAG（Directed Acyclic Graph，有向无环图），DAG 中包含顶点和边。Spark 的 RDD 就是根据这个 DAG 进行流转计算，DAG 不仅可以帮助 Spark 进行一些计算的优化，还可以进行容错恢复。Flink 也有类似的设计，只不过不叫 DAG，而是叫 DataFlow Graph，叫什么不重要，重要的是理解其概念和原理。 Flink 的执行图分为下面四层，StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图，每层都是从上一层转换优化而来。 StreamGraph：最初始的图，是根据我们用 DataStream API 编写的代码转换而来 JobGraph：对 StreamGraph 进行优化（Operator Chain）得到 ExecutionGraph：在 Flink Cluster 中 JobManager 根据 JobGraph 并行化处理得到 物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。 为什么要这四张图，我理解的是为了屏蔽上层差异，并且层与层之间进行解耦，Flink 支持流和批处理，但是其 API 是不一样的，流处理使用 DataStream API；批处理使用 DataSet API。 上图中 DataStream API 先转换成 StreamGraph 在转成 JobGraph；DataSet API 先转换成 OptimizedPlan 再转成 JobGraph，为啥不直接转成 JobGraph 呢？因为针对批处理在执行前可以做很多预分析用来优化图的执行，但是这些优化并不能用于流处理。所有就有了 StreamGraph 和 OptimizedPlan 进行过度。 而 JobGraph 则统一了 StreamGraph 和 OptimizedPlan，并进行了适用两者（流与批）的优化（Opertor Chain），再下层的 ExecutionGraph 是 JobGraph 的并行化版本，最下层的物理执行图则是实际运行在 TaskManager 中 Task 组成的“图”。 我们目前只关注 Flink 的流处理部分，所有先来看看 StreamGraph 是怎么生成的，不过在聊 StreamGraph 之前需要先了解几个概念 DataStream下面引用代码中的注释 A DataStream represents a stream of elements of the same type. A DataStream can be transformed into another DataStream by applying a transformation as for example: map filter DataStream 是对相同类型的元素流进行封装，并且可以通过 Transformation 转换成其它 DataStream，目前 DataStream 有下图这些实现，SingleOutPutStreamOperator（类名起得有点问题，容易和之后的 StreamOperator 混淆）、KeyedStream、IterativeStream、DataStreamSource。 TransformationDataStream 是面向用户的，以简化数据处理，而 Transformation 是面向 Flink 内核的。调用 DataStream API 数据处理流水线都会转换为 Transformation 流水线，如下图所示 Transformation 主要分为两类：物理 Transformation(PhysicalTransformation) 和虚拟 Transformation，DataStream API 的调用都会转换为 Transformation，然后从 Transformation 转换为实际运行的算子，而虚拟的 Transformation 则不会转换成实际的算子。如 Reblance、Union、Split、Select 就不会转换成实际算子。 下图仅仅是一部份 Transformation，详细请见 DataStream Transformations Trasformation 抽象类是最顶层的抽象，所有物理 Transformation 均继承 PhysicalTransformation，其它虚拟 Transformation 均继承 Transformation 抽象类。 Transformation 中定义了下面这些比较重要的属性： id：一个累加 id name：Transformation 的名称，主要用于可视化 uid：用户指定的 id，主要用来在任务重启的时候可以再次分配与之前相同的 uid，以持久保存状态 parallelism：并行度 outputType：输出类型，主要用于序列化数据 PhysicalTransformation 主要可以分为四类： SourceTransformation：读取数据源的 Transformation，这个 Transformation 没有上游，只有下游，是 Flink 作业的起点，一个 Flink 作业可以有多个 SourceTransformation，从多个数据源读取数据，如多流 Join、维表 Join、BroadcastState 等场景 SinkTransformation：输出数据的 Transformation，只有上游，下游就是外部系统，如 HDFS、Kafka 等，是 Flink 作业的终点。一个作业可以有多个 SinkTransformation，可以输出到不同的外部系统 OneInputTransformation：单流输入的 Transformation，构造器与 SinkTransformation 相同，都需要 input 和 operator。 TwoInputTransformation：双流输入的 Transformation，有两个输入流。 虚拟 Transformation，下面介绍几个比较常见的虚拟 Transformation: SideOutputTransformation：旁路输出 Transformation，表示上游的一个分支，上游可以有多个下游 SideOutputTransformation UnionTransformation：主要用于合并多个流 PartitionTransformation：用于改变输入元素的分区，工作时除了提供一个StreamTransformation作为输入外，还需要提供一个StreamPartitioner的实例来进行分区。 说了这么多还是没搞明白 Transformation 到底是什么，可能单说 Transformation 不好理解，那看看下面这张图是不是会好理解些。 我们从最里层看，也就是 MapFunction，这里就是 UDF(user defined function) 我们编写的一些函数实现，如 Map、Filter 实现等等，然后就是对 UDF 进行封装的 StreamOperator 层，StreamOperator 是对 UDF 的一层封装，在 StreamOperator 中定义了 UDF 的生命周期，然后最下层就是 Transformation 了，Transformation 是对 StreamOperator 的封装。每个 Transformation 都会记录其前一个 Transformation，因此就形成了一个 Transformation 链。 StreamOperator算子在 Flink 流处理中叫 StreamOperator，每个 Task 有一个或者多个算子，每个算子代表一个计算步骤，具体的计算逻辑由封装的 Function 来实现，StreamOperator 还定义了算子的生命周期，下面来看下其接口代码 下面是几个比较关键的抽象类和接口 AbstractStreamOperator：所有 Stream Operator 的基类，定义了一些配置、运行时参数、kv state 操作等等 AbstactUdfStreamOperator：封装一个 UDF，但是主要操作都是交给 UDF 操作，如 open、close 等 OneInputStreamOperator：表示只有一个输入的流，实现这个接口需要实现一个 processElemnet() 方法，并在其中实现处理逻辑，map、FlatMap、Filter 等等都实现了这个接口 TwoInputStreamOperator：两个输入的流，实现这个接口需要实现一个 processElemnet1() 和 processElemnet2() 方法，并在其中实现处理逻辑，双流 Join 操作就是实现这个接口 MultipeInputStreamOperator：多输入流，OneInputStreamOperator 的 List 版本。 Flink 算子的设计分为两条线，与数据处理相关的主要由 OneInputStreamOperator 和 TwoInputStreamOperator 接口定义；与生命周期、状态处理相关的由 AbstractStreamOperator 抽象类定义实现，AbstractStreamOperator 还定义了算子融合策略 ChainingStrategy，之后的文章会提到。 StreamOperator 基本上与 DataStream API 相对应，例如，map 对应 StreamMap、flatMap 对应 StreamFlatMap、filter 对应 StreamFilter 等等。StreamOperator 大致也可以分为四类： 单流输入算子 双流输入算子 数据源算子 异步算子 重点解释下异步算子，异步算子主要是解决同步调用外部系统等待导致系统吞吐量降低的问题。下图展示同步算子和异步算子的区别 既然是异步调用，那就有调用请求返回先后的顺序，比如后调用的请求先返回，那此时这个元素输出还是需要等其前面的元素处理完再输出？Flink 定义了两种输出模式： 顺序输出模式： 先收到的数据元素先输出，后续数据元素的异步函数调用无论是否先完成，都需要等待。顺序输出模式可以保证消息不乱序，但是可能增加延迟、降低算子的吞吐量 无序输出模式： 简单来讲，就是先处理完的数据元素先输出，不保证消息顺序，相比顺序模式，无序输出模式算子延迟低、吞吐量更高。无序输出并不是完全无序的，其按照 Watermark 进行分组，保证组内无序，组间严格有序。也就是在 Watermark 之后的元素请求完成了也要等 Watermark 前面的元素处理完了才能输出。 Funciton函数在 Flink 中就叫 Function，我们自己编写函数叫 UDF（User Defined Function），Flink 内置了一些预定义可供我们直接使用的 UDF，如 sum、max、join 等等。按照输入输出可以将函数分为以下三类： SourceFunction：数据源函数，没有上游 Function，一般直接从外部系统读取数据。 SinkFunction：数据汇函数，没有下游 Function，一般将数据直接写入外部系统。 一般函数：处于整个函数链中间的函数，有上游也有下游。 函数的层次： UDF 一般在 DataStream API 使用，从 Flink 提供的函数体系接口来看，可以按下图层级分层： 无状态函数 我们常用的 DataStream#map、DataStream#faltMap、DataStream#filter 都是高阶函数，我们使用高阶函数的时候无须关系触发器等底层特性，只需关注函数逻辑即可。无状态函数用来做无状态计算，比较简单，如 MapFunction，无状态函数与 RichFunction 是一一对应的，如 MapFunction 和 RichMapFunction。我们使用状态函数时只需要实现对应的无状态接口或者通过匿名函数的方式实现其逻辑即可。 RichFunction RichFunction 相较于无状态函数，增加了两个特性： 增加了生命周期管理，增加了 open()、close() 方法。在作业启动时，Function在open方法中执行初始化，在Function停止时，在close方法中执行清理，释放占用的资源等。无状态Function不具备此能力。 增加了 getRuntimeContext() 和 setRuntimeContext()，通过 RuntimeContext 可以获取执行时作业级别的的参数信息。 无状态函数天然就时容错的，作业失败之后重新执行即可。但是有状态函数 RichFunction 需要处理中间结果和状态的保存，持有了访问状态的能力，待故障重启后只需要从失败前保存的状态中恢复即可。 处理函数 处理函数 ProcessFunction 可以访问流处理应用的所有基本组件： 事件：数据流中的元素 状态 定时器 KeyedProcessFunction 与 Non-Keyed ProcessFunction 的区别是 KeyedProcessFunction 只能作用在 KeyedStream 上；ProcessFunction 与 CoProcessFunction 的区别就是 CoProcessFunction 是双流输入，ProcessFunction 是单流输入。出了输入函数还有广播函数、异步函数、数据源函数、数据汇（输出）函数，就不一一解释了，感兴趣的朋友可以自行去看下代码和接口实现结构 广播函数 异步函数 总结： 本来这篇是将 StreamGraph 的生成流程的，但是还是先把一些前置的概念弄清楚。本篇介绍了 Flink 的四层 Graph 的区别以及为什么要分层设计 Graph，然后讲到了 DataStream、Transformation、StreamOperator、Function 等概念。了解到 DataStream API 是面向用户的，而 Transformation 是面向 Flink 内核的。然后讲了 Transformation -&gt; StreamOperator -&gt; Function 的封装层级。最后提到了 Flink 的函数体系从高阶到低阶的过程，无状态Function -&gt; RichFunction -&gt; ProcessFunction。"},{"title":"Flink-1.14 源码分析--任务提交流程","date":"2022-01-09T03:45:09.000Z","url":"/2022/01/09/Flink%20%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/","tags":[["Flink","/tags/Flink/"],["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["源码分析","/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"]],"categories":[[" ",""]],"content":"Flink 任务提交的大体流程如下图所示，通过 CliFrontend 运行用户 Jar 中的 main 方法，再通过 PipelineExecutor 将任务提交到 Flink 集群 下面具体分析任务提交的代码。 通常我们使用 flink-dist/src/main/flink-bin/bin/flink 脚本提交 Flink 任务，该脚本会运行 org.apache.flink.client.cli.CliFrontend。在 CliFrontend 中根据不同的参数执行不同的 action，如运行任务、取消任务、停止任务、执行 savepoint 等等 重点看下运行任务，即 run() 方法。在 run() 方法中会进行任务提交参数校验，获取任务依赖的 jar 包，将应用 Jar 包封装成 PackageProgram 对象，这个对象会加载提交参数中的主类。 通过 executeProgram() 方法，最后会通过 org.apache.flink.client.ClientUtils#executeProgram 调用 PackagedProgram 对象的 invokeInteractiveModeForExecution() 方法调用我们编写任务的 main 方法。至此 CliFrontend 的使命就完成了，将接力棒交到了 custom main 手中。 我们以 org.apache.flink.streaming.examples.wordcount.WordCount 官方示例为例，我们一般编写流任务第一步都需要获取 StreamExecutionEnvironment，最后再调用 StreamExecutionEnvironment() 方法触发任务执行 既然任务是以 env.execute() 方法触发，那就将 execute() 作为入口，看看后续的逻辑。 上面代码分为两部分，第一 execute() 方法会获取一个 StreamGraph 对象（StreamGraph 这部分后面会单独写一篇，现在只需要将它视为一个 “符号引用”，表示我们编写代码的逻辑即可）；第二个 execute() 方法接上面获取的 StreamGraph 对象作为参数，再调用 executeAsync() 方法就返回了一个 JobClient 对象，然后通过 JobClient 对象获取任务的执行结果。显然，executeAsync() 就是任务提交执行的重点，but，在介绍 executeAsync() 方法前，我们先看下获取任务执行结果的两个分支。 从变量名上可以看出部署分为 Attached 和 Detached 两种模式，即我们提交任务后 Client 是否可以退出。Detached 模式下，Client 在提交完任务后就可以退出了，任务运行在 Flink 集群上不需要管了；而 Attached 则在 Client 提交任务后不能够退出，即不能关闭命令行窗口，需要继续与 Flink 集群保持连接，能够实时看到集群任务的一些运行日志和运行结束状态等等。 ok，插播完毕，继续来看看 executeAsync() 方法，从方法名上就能看出来任务是异步提交的，废话少说，上代码 诶！上面代码出现了 PipelineExecutorFactory 跟开局第一张图中的 PipelineExecutor 有点像，没错，就是 PipelineExecutor 的工厂类，是通过 SPI 的方式，不同类型的集群以插件的方式实现，方便以后扩展，通过任务提交时的参数进行匹配，如果没有配上一个或者匹配多个，都会抛出异常。PipelineExecutorFactory 会获取一个 PipelineExecutor 对象，再执行其 execute() 方法。下面插播一下 PipelineExecutor。 代码里关于 PipelineExcutor 的注释为 The entity responsible for executing a Pipeline, i.e. a user job. 负责执行 Pipeline 的实体，比如一个用户任务。 首先解释下 Pipeline，从我们编写的代码来理解，我们 text.flatMap(new Tokenizer()).keyBy(value -&gt; value.f0).sum(1); 从输入一直点点点调用一些函数，然后这些函数对数据进行处理，是不是有点像我们的工厂流水线？这里先这么理解，后续关于 StreamGraph 时再详细解释。Pipeline 是一个接口，有两个实现 Plan 和 StreamGraph 分别表示批处理和流处理。 现在再来看 PipelineExecutor，PipelineExecutor 也是一个接口，有很多的实现 AbstractJobClusterExecutor、AbstractSessionClusterExecutor、LocalExecutor、EmbededExecutor PipelineExecutor 是将我们的 Flink Job 提交给 Flink 集群的重要环节。Flink 集群有两种模式 Session、Per-Job 模式，这两种模式的集群启动时间、提交作业的方式都不太一样，所以上面类图中有两种 Executor，AbstractSeesionClusterExecutor、AbstracJobClusterExecutor 分别对应 Session 和 Per-Job 模式。 出了两种集群模式外还有一个 LocalExecutor，会在我们本地测试时运行 Flink MiniCluster 时用到。而剩下的 EmbeddedExecutor 则是一个内嵌直接调用 Dispatcher 的 Executor，而不是通过 REST API 调用。这是说了直接调用而不是使用 REST API 调用，那肯定有些 executor 是通过 REST API 调用了，没错就是 Session 模式下的 AbstractSessionClusterExecutor。下面分别聊聊 Session 模式和 Per-Job 模式任务提交的异同 Session 模式 Session 模式下，提前启动一个 Flink 集群，所有作业共享同一个集群的资源，作业通过 HTTP 协议提交 AbstractSeesionClusterExecutor 有三个子类，KubernetesSessionClusterExecutor、YarnSeesionClusterExecutor、RemoteExecutor，很明显 Flink 集群的 Session 模式支持 k8s 和 yarn，而 RemoteExecutor 表示 Standalone。前两者都是弹性可伸缩的，而 Standalone 比较特殊，在机器上直接部署 Flink，那么就已经启动了一个集群，天生就是 Session 模式且不支持 Per-Job 模式。 Session 模式下，任务提交通过 yarn-session.sh 脚本，k8s 则通过 kubernetes-session.sh 提交，两者逻辑基本相同。都是在提交任务前会先检查是否已经存在已经启动的 Flink Session 集群，如果没有则先启动一个 Flink Session 集群，然后在 PipelineExecutor 中通过 ResetClusterClient 向 Dispatcher 提交任务，然后 Dispatcher 为这个任务启动一个 JobMaster，进入作业执行阶段。具体代码逻辑就不贴了，有兴趣的朋友可以自行去看下。下面给一张图作为参考 Per-Job 模式 目前只有 Yarn 集群支持 Per-Job 模式，在 Per-Job 模式中，不会共享 Flink 集群，每次提交一个任务都会单独启动一个 Flink 集群，我们来分析下 AbstractJobClusterExecutor 上面流程中，先获取 JobGraph 将之前的 StreamGraph 转换成 JobGraph，然后获取一个 ClusterDescriptor 集群描述符，是部署群集(例如Yarn)并返回用于群集通信的客户端的描述符。最后通过 ClusterDescriptor.deployJobCluster() 部署任务到集群中。 很明显 ClusterDescriptor 一定是一个接口或者抽象类，针对不同的集群有不同的实现方式。后续代码就不贴了，无非就是检查配置信息，Yarn 集群信息，然后再将一些必要的文件如 Jar 包、log 配置文件、flink 配置文件上传到 HDFS，然后再通过 YarnClient 提交启动一个 Application。有兴趣的朋友可以自行去看下代码，代码在 org.apache.flink.yarn.YarnClusterDescriptor#startAppMaster。最后再贴一下 Per-Job 的提交流程图，关于任务提交到 Yarn 之后的部分会在以后的资源管理和作业调度中再详细分析。 最后再提一下通过 ./bin/flink run 是 Per-Job 方式提交，在 Client 端生成 JobGraph；如果通过 ./bin/flink run-application 通过这个命令提交参数，则以 application 方式提交，区别是我们编写的 main() 方法将在 JobManager 上运行，也就是说会在 JobManager 上生成 JobGraph。 总结： 本文分析了 Flink 作业提交时在 Client 端的流转流程，分析了一个作业时如何提交到 Flink 集群中的，并分析了 Session 模式与 Per-Job 模式的异同。在 Client 端，CliFrontend 是作业提交的入口，该类会通过反射的方式运行我们编写的类中的 main() 方法，之后在 StreamExecuteEnvironment 中会生成 JobGraph，最后再将 JobGraph 提交到集群中运行。"},{"title":"SparkSQL","date":"2021-12-08T12:27:00.000Z","url":"/2021/12/08/SparkSQL/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"SparkSQL SparkSQL总体设计Spark SQL 总体设计如下图所示，主要包含以下几个模块 Parser Analyzer Optimizer Spark planner Code Generator 其中 Spark SQL 的核心是 Catalyst 优化器，主要有两个方向，基于规则优化（Rule Based Optimizer/RBO）、基于代价优化（Cost Based Optimizer/CBO） ParserSpark SQL 语句和 DataFrame 通过 ANTLR4 生成两个类，一个 SqlBaseLexer（词法解析器）一个 SqlBaseParser（语法解析器）。使用这两个解析器将 SQL 字符串语句解析成了 ANTLR4 的 ParserTree 语法树结构，再使用 AstBuilder.scala 将 ParserTree 转换成 catalyst 表达式逻辑计划 LogicPlan。 Analyzer经过第一步之后，已经有了语法树，但是语法树的组成都是一个一个的占位符，并不知到其具体代表什么，例如上图的 people、score、sum 等。这时就需要一些元信息来表达这些占位符，主要是两部分：表的 scheme 和基本的函数信息，表的 scheme 主要包括基本定义（列名、数据类型）、表的数据格式（Json、text、parqut）、表的物理位置等。基本函数信息主要指类信息。 Analyzer 会再次遍历语法树，对树上的每个节点继续数据类型和函数绑定，例如 people 占位符会根据元数据表信息解析为包含 age、name、id 的三列的表，people.age 会被解析为 int 的变量，sum 会被解析为特定的聚合函数等 Spark SQL 中 Analyzer 中定义了各种 Rule（解析规则），多个性质类似的 Rule 组成一个 Batch，多个 Batch 构成一个 batches，这些 batches 会由 RuleExecutor 执行，先按一个一个 Batch 顺序执行，然后对 Batch 里面的每个 Rule 顺序执行。每个 Batch 会执行一次（Once）或多次（FixedPoint，由 spark.sql.optimizer.maxIterations 参数决定），执行过程如下： 下面贴出了一些 Rule 整体来看 Analyzer 就是完成以下工作 确定最终返回字段名称以及返回类型 确定聚合函数 确定表当中获取的查询字段 确定过滤条件 确定join方式 确定表当中的数据来源以及分区个数 Optimizer优化器是 Catalyst 的核心，分为 RBO（基于规则的优化器） 和 CBO（基于代价的优化器） 两种 RBO：基于规则的优化器其实就是对语法树进行一次遍历，根据模式匹配能够满足特定规则的节点，进行等价转换。因此 RBO 就是将一棵树转换成另一棵树，常见的优化规则有：下推优化、常量累加优化、列值剪裁 下推优化最常见的就是谓词下推，将过滤逻辑尽量靠近数据源，减少后续操作的数量 常量累加，就是将一些常量运算直接算出结果进行替换，以免后续每条结果都要进行计算。 列剪裁，只保留查询需要的列，不扫描那些不需要的列，例如下图中的 People，只需要 id，所以在扫描 people 之后需要将其他列进行裁剪，只留下列 id。这个优化一方面大幅度减少了网络、内存数据量消耗，另一方面对于列存数据库（Parquet）来说大大提高了扫描效率。 下面是常用的优化规则 SparkPlanner至此，可以得到一个经过优化后的执行计划（Optimized logical plan）,但是 OLP 还没办法真正执行，只是逻辑上可行，Spark 不知道如何去执行这个 OLP。例如，join 只是一个抽象的概念，代表两个表根据相同的 id 进行合并，然而具体怎么合并，OLP 并没有说明。这时就需要将 OLP 转换为 physical plan 物理执行计划，将逻辑上可行的执行计划变为 spark 可以真正执行的计划。下图表示将一个 OLP 转换为 Physical Plan 的过程。 上图可以看到，将 OLP 的一些节点都转换为 Spark 的算子，比如 join 操作，Spark 根据不同的场景为该节点算子不同的算法策略，有 BroadcastHashJoin、ShuffleHashJoin、SortMergeJoin，物理执行计划实际上就是在这些具体实现中挑选一个耗时最小的实现，这个过程就涉及到 cost base optimizer/CBO。 最后这个转换是由 SparkPlanner 来完成的，SparkPlanner 继承了几个抽象类，其关系如下： 在 QueryPlanner 中实现了一个 plan() 方法，这个方法通过对一些 SparkStrategy 进行替换 OLP 中的占位符，可以将 OLP 转换为物理执行计划。SparkStrategy 主要可以分为三类： experimentalMethods 的 extraStrategies extralPlanningStategies 策略，该策略列表会在 sessionState 时创建 常规策略（GenericStrategy）,这些策略都继承自 SparkStrategy，对应不同的操作，如 join、limit 等。主要有以下这些： SparkPlanner 实现： 通过配置（spark.sql.shuffle.partitons，默认 200）拿到 shuffle 的分区数。 设定 SparkStrategy，形成一个策略列表，保存到 strategies 中。 定义一个 prunePlans 函数，通过该函数来防止组合爆炸，需要修剪坏的计划，但是在 spark-2.3 中什么也没做，后续应该会实现。 提供一个 pruneFilterProject 函数，用于构建表扫描符，使用单独的物理运算符来完成复杂的映射和过滤，可能会添加 project 和 Filter 节点，主要是映射下推（列裁剪）优化 SparkPlanner 的父类：SparkStrategies SparkStrategies 定义了各种将 OLP 转换为物理计划的的策略，主要包含以下几种策略： limit operators：SpecialLimits join operators：JoinSelection streaming aggregation：StatefulAggregationStrategy streaming deduplicate：StreamingDeduplicationStrategy streaming join：StreamingJoinStrategy aggreation：Aggregation in memory scans：InMemoryScans stream relation：StreamingRelationStrategy FlatMapGroupsWithStateStrategy 基本操作（distinct、intersect 等等）：BasicOperators 最上层抽象类：QueryPlanner 其核心就是 plan() 方法，逻辑如下： 收集物理执行计划候选集 需要转换的候选集中可能包含占位符，需要用他们的自节点替换该占位符 通过 prunePlans() 方法来修剪物理执行计划，当前版本（2.3）啥也没做 至此，物理执行计划就生成了，但是当前的物理执行计划并不是最优的，后续还要在 prepareForExecution 中进行优化，才能进行执行代码的生成。 Code Generator目前为止，Spark 已经生成一份物理执行计划，但是还不能直接执行，还需要用一些 Rule 对 SparkPlan 进行处理，也就是 prepareForExecution 过程，有如下这些 Rule。 CollapseCodegenStages 是核心部分，代码生成阶段。一般处理 SQL 语句都是通过一个叫 Volcano Iterator Model（参见 《Volcano-An Extensible and Parallel Query Evaluation System》）的模型，在 Spark 2.0 以前就是基于这个模型处理 SQL 的，当今大多数的数据库系统处理 SQL 在底层都是基于这个模型，大概逻辑为： 数据库引擎将 SQL 翻译成一系列的关系代数或表达式 依赖这些关系代数或着表达式逐条处理输入数据并产生结果 每个算子在底层都实现同样的接口，比如 next() 方法，然后顶层算子调用子算子的 next()，一层层调用，如下图： Volcano Iterator Model 的优点是抽象起来很简单，容易实现，可以通过任意组合来表达复杂的查询，缺点就是性能差。databricks的官方博客对比过使用 Volcano Iterator Model 和模拟手写代码的执行效率，结果发现模拟手写的代码执行效率要高出十倍！所以总结起来就是将 SQL 解析成为代码，比 SQL 引擎直接解析 SQL 语句效率要快，所以spark2.0 最终选择使用代码生成的方式来执行 SQL 语句。 代码生成分为三部分： 表达式代码生成（expression codegen） 全阶段代码生成（Whole-stage Code Generation） 代码编译 表达式生成 表达式代码生成的基类是 CodeGenerator，有七个子类 例如 age &gt; 10 就是最基本的表达式，也是一个 Predicate，所以会调用 GeneratePredicate 来生成表达式的代码。表达式代码生成主要是想解决大量虚函数调用（Virtual Function Calls），泛化的代价等。需要注意的是，通过表达式生成完整的类代码只有在将 spark.sql.codegen.wholeStage 设置为 false 才会进行的，否则只会生成一部分代码，并且和其他代码组成 Whole-stage Code。 全阶段代码生成 全阶段代码生成，用来将多个处理逻辑整合到单个代码模块中，也会用到上面的表达式生成。不过和表达式生成不同的是，这是对整个 SQL 过程进行代码生成，表达式生成仅仅是针对表达式。 全阶段代码生成的代码都是继承自 BufferedRowIterator，生成的代码需要实现 processNext() 方法。processNext() 方法在 WholeStageCodegenExec 里面的 doExecute 方法里面被调用。而这个方法里面的 rdd 会将数据传进生成的代码里面。调用链如下图所示 生成的代码很好理解，以 age != null and age &gt; 10 为例，其实就是对每行的 age 进行 isnotnull(age#8) &amp;&amp; (age#8 &gt; 10) 表达式判断，然后拿到符合条件的行。 相比之前的 Volcano Iterator Model，全阶段代码生成的执行过程如下： 代码编译 生成代码之后需要解决的另一个问题是如何将生成的代码进行编译然后加载到同一个 JVM 中去。在早期 Spark 版本是使用 Scala 的 Reflection 和 Quasiquotes 机制来实现代码生成的。Quasiquotes 是一个简洁的符号，可以让我们轻松操作 Scala 语法树，具体参见 这里。虽然 Quasiquotes 可以很好的为我们解决代码生成等相关的问题，但是带来的新问题是编译代码时间比较长（大约 50ms - 500ms）！所以社区不得不默认关闭表达式代码生成。 为了解决这个问题，Spark 引入了 Janino 项目，参见 SPARK-7956。Janino 是一个超级小但又超级快的 Java™ 编译器. 它不仅能像 javac 工具那样将一组源文件编译成字节码文件，还可以对一些 Java 表达式，代码块，类中的文本(class body)或者内存中源文件进行编译，并把编译后的字节码直接加载到同一个 JVM 中运行。Janino 不是一个开发工具, 而是作为运行时的嵌入式编译器，比如作为表达式求值的翻译器或类似于 JSP 的服务端页面引擎，关于 Janino 的更多知识请参见这里。通过引入了 Janino 来编译生成的代码，结果显示 SQL 表达式的编译时间减少到 5ms。在 Spark 中使用了 ClassBodyEvaluator 来编译生成之后的代码。 需要注意的是代码生成是在 Driver 端，而代码编译是在 Executor 端。 最后就是 SQL 执行了 总结：本文总体讲述了在 Spark SQL 中一条 SQL 语句是如何在 Spark 中执行的，经历了 Analyzer、Optimizer、SparkPlanner 这几个流程，但还有一些点没有涉及到，例如：CBO（cost base optimizer）,后续补上。 参考资料SparkSQL – 从0到1认识Catalyst Spark SQL Catalyst优化器 spark2 sql原理分析–逻辑计划转换成物理计划的实现分析(SparkPlanner) 详解Spark SQL 底层实现原理(parser、analyzer、optimizer、physical plan) 一条 SQL 在 Apache Spark 之旅（下） "},{"title":"Spark 存储系统1—块信息","date":"2021-12-01T12:27:00.000Z","url":"/2021/12/01/Spark%20%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F1%E2%80%94%E5%9D%97%E4%BF%A1%E6%81%AF/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"Spark 中有个重要的组件 BlockManager（块管理器），负责 Spark 存储管理。由名称可知，Spark 的存储系统的基本单位是由 Block ，那我们就从 Block 入手，学习一下 Spark 的存储系统。 块主要包含三个主要的实现，块 id，块数据和块元信息，分别对应三个类 BlockId、BlockData 和 BlockInfo BlockId： 先来看下块 id，块 id 与 RDD id 不同的一点是，RDD id 是一个整数，而块 id 是一个类。BlockId 的抽象类如下 除了块名称之外还有三个 isXXX 方法，判断当前的 BlockId 是否为 RDDBlockId、ShuffleBlockId、BroadcastBlockId，这三个类都是 BlockId 的主要实现类。 三个实现类也只是覆盖了 name 为不同的组合方式，而 BlockId 的伴生对象中也是根据 name 的生成方式区分不同的实现类 BlockId 只是封装了 Block 的名称，并没有封装数据，Block 的数据封装在 BlockData 中，BlockData 是一个特质（接口）类，定义一些接口。 BlockData 中定义了一些转换接口，特别说明一下 ChunkedByteBuffer 是由多个 ByteBuffer 包装而成，表示多个不连续的内存缓冲区数据。 BlockData 有三个实现类，分别是基于内存和 ChunkedByteBuffer 的 ByteBufferBlockData，基于磁盘的 DiskBlockData 和加密的 EncryptedBlockData 以 ByteBufferBlockData 为例，下面是 ByteBufferBlockData 的代码 可以看出 ByteBufferBlockData 就是代理了 ChunkedByteBuffer 所以再看一下 ChunkedByteBuffer ChunkedByteBuffer 的属性 ChunkedByteBuffer 的构造方法参数是一个 Array[ByteBuffer]，也就是说，其实 ChunkedByteBuffer 就是一个 ByteBuffer 数组，即一个 chunk 就是一个 ByteBuffer。 另外还提供了一个 writeFully 方法将数据以 bufferWriteChunkSize 大小写入 NIO Channel 其它方法以后碰到了再了解。 最后还剩一个块元信息——BlockInfo 先上代码 除了 level、classTag、tellMaster 三个属性外，BlockInfo 还定义了三对 getter/setter 方法，分别是 size：Block 的大小，单位为字节 readerCount：读取当前块的 task 个数，用于加读锁 writerTask：写当前块的 task，用于加写锁 BlockInfo 的锁是由 BlockInfoManager 管理的，之后再一起学习。 参考资料 Spark Core源码精读计划21 | Spark Block的基本实现"},{"title":"Spark 存储系统2—BlockInfoManager 读写锁管理","date":"2021-12-01T12:27:00.000Z","url":"/2021/12/01/Spark%20%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F2%E2%80%94BlockInfoManager%20%E8%AF%BB%E5%86%99%E9%94%81%E7%AE%A1%E7%90%86/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"之前我们学习了 BlockId、BlockData 和 BlockInfo，在 BlockInfo 中有两个属性：readerCount 和 writerTask 分别对应读当前 Block 的 task 数量和写当前 Block 的 task 。这是一个 Spark 实现的读写锁机制，由 BlockInfoManager 来管理读写锁。 先看下 BlockInfoManager 有哪些属性 在实例化 BlockInfoManager 时会调用 registerTask 方法添加一个特殊尝试 id 到读锁，标识当前块的写锁被非任务线程持有 来看看写锁是怎么实现的 写锁上锁过程： 先判断当前 infos 映射关系中有没有保存这个 block 判断当前块有没有上写锁 block 的 readercount 加一，保存 task 与块的读锁关系，释放锁时用到 如果当前块被上了写锁，且是阻塞模式就会一直等待；否则直接返回 None 再看下读锁上锁过程 写锁的上锁流程与读锁基本一直，只不过能上写锁的条件为： 当前块没有被上写锁 当前块的读锁为 0 所以可以知道，读写、写写互斥，读读不互斥，且读锁是可重入的。 锁释放 释放锁流程： 先获取 task attempt id 如果是写锁则将 block 的写锁 task 置为 NO_WRITER，并在当前写锁记录中移除锁记录 如果是读锁，block 的读锁记录 -1，因为是可重入锁所以将 task 对应这个 block 的写锁 -1 唤醒其它线程争抢锁 BlockInfoManager 还提供了一个释放 task 持有的所有锁方法，很简单，有兴趣可以看下 releaseAllLocksForTask 锁降级，这里的锁降级是指将写锁降级成读锁 可以看出锁降级很简单，就是先加写锁释放，在获取读锁的过程。 新写入一个块也需要加写锁，防止同时写的情况 BlockInfoManager 还有两个移除块信息的方法，分别是 removeBlock 和 clear 方法 clear 方法是移除所有 block 且不需要持有写锁，感兴趣的朋友可以看下。 总结： BlockInfoManager 从名称来看是 Block 信息管理，其实更主要的是锁管理。管理着读写锁的获取，释放等。"},{"title":"Spark 存储系统3—内存管理","date":"2021-12-01T12:27:00.000Z","url":"/2021/12/01/Spark%20%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F3%E2%80%94%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"Spark 是基于 JVM 的应用程序，所以按照 JVM 层面可以把 Spark 的内存分为堆内（上）内存（on-heap）和堆外内存（off-heap）。如果按照 Spark 对内存管理层面区分的话，可以分为执行内存（Execution Memory）和存储内存（Storage Memory）。执行内存主要是存放 shuffle、join、sort 和 aggregation 操作数据；而存储内存主要存放 cache 和 broadcast 等缓存数据。 这次来了解一下 Spark 是如何管理内存的。与内存存储相关的组件包括内存池 MemoryPool、内存管理器 MemoryManager、内存存储器 MemoryStore。本文先来探索内存池和内存管理器的大体实现。先来看下 MemoryPool 可以看到 MemoryPool 仅仅定义了一个 _poolSize 表示 JVM 分配给 Spark 的内存大小，然后就是定义两个增加和减少可用内存的方法，这个抽象方法只是定义了如何记录增加和减少内存，具体可用内存和增加/减少的内存大小还是得看实现类 ExecutionMemoryPool 、StorageMemeoryPool对应存储执行内存管理和内存管理。 看下具体实现类 StorageMemoryPool 申请空间 上面的大体流程为： 一个 block 需要申请存储空间 判断空闲空间是否够这次空间申请，如果不够就通过 memoryStore#evictBlocksToFreeSpace 释放出 numBytes - memoryFree 空间 已使用空间数据记录 _memoryUsed 增加 释放空间 释放空间就更加简单了，直接将 _memoryUsed 已使用空间减去需要释放的空间即可 最后来看下 MemoryManager，MemoryManager 也是一个抽象方法，先看下有哪些属性 MemoryManager 通过存储内存、执行内存与堆内、堆外内存组合出四种存储池，分别是堆内存储内存、堆外存储内存、堆内执行内存、堆外执行内存。 分配初始内存时堆内是通过传入的参数确定的，而堆外则是通过配置进行计算的，这里涉及两个参数 spark.memory.offHeap.size 可用堆外内存总数（默认 0）spark.memory.storageFraction 堆外存储内存系数（默认 0.5）。先计算出堆外存储内存的大小，再通过堆外可用内存减去堆外存储内存得到堆外执行内存的大小 MemoryManager 中定义了 acquireStorageMemory、acquireUnrollMemory、acquireExecutionMemory 三个抽象方法，分配对应分配存储内存、Unroll 内存、执行内存这些都交由具体的子类去实现。这里解释一下 Unroll 内存，RDD在被缓存（cache）之前，它所占用的内存空间是不连续的，而被缓存到存储内存之后，就以块的形式来存储，占用连续的内存空间了。Unroll就是这个将RDD固化在连续内存空间的过程，中文一般翻译为“展开”。Unroll过程使用的内存空间就是展开内存，它本质上是存储内存中比较特殊的一部分。 而释放空间则基本上是代理了 MemoryPool 就不再赘述。 待补充 ExecutionMemory unroll Tungsten 计划 参考资料 Spark Core源码精读计划#23：与存储相关的内存池及内存管理器的具体实现"},{"title":"Spark 存储系统4—统一内存管理器","date":"2021-12-01T12:27:00.000Z","url":"/2021/12/01/Spark%20%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F4%E2%80%94%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%99%A8/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"前一篇提到了 Spark 的内存管理器 MemoryManager，这篇将展开叙述 MemoryManager 的实现 UnifiedMemoryManager。由于目前版本（spark-3.0）只有统一内存管理器，所以不讨论静态内存管理器，关于静态内存管理器和统一内存管理器的区别可以参考Spark 统一内存管理器 按照惯例，先看属性和构造方法 属性就是 MemoryManager 中的属性，那就看看伴生对象的 apply 构造方法没什么特殊就是通过一个 getMaxMemory 方法获取 Spark 可用（存储内存 + 执行内存）最大内存，以及通过 spark.memory.storageFraction（默认 0.5） 配置堆上存储内存的比例。重点看下这个 getMaxMemory 方法 getMaxMemory 的流程为： 获取 JVM 中的堆空间大小 计算预留空间（默认 300MB） 校验系统内存与指定 Executor 内存是否比要求的最小内存大（450 MB） 计算可用内存（系统内存 - 预留内存） 计算可用与存储内存和执行内存的大小（可用内存 * 0.6） 可能有点乱，又是系统内存，又是预留空间等，可用参考下面这张图来理解 需要注意的一点是不管是堆内还是堆外我们只能通过配置参数设置 StorageMemory，然后在通过计算得到 ExecutionMemory 看完了构造方法，再来看下实现 MemoryManager 的三个方法：acquireStorageMemory、acquireUnrollMemory 和 acquireExecutionMemory 先看 acquireStorageMemory 大概流程： 根据内存模式判断这次分配的是堆内还是堆外内存 判断剩余内存是否足够这次分配，如果够则直接分配了 如果不够需要向 ExecutionMemroy 尝试借用内存，不过只借不够的部分即 numBytes - memroyFree 如果 ExecutionMemroy 也不够的话，只能通过前一篇文章中的 memoryStore#evictBlocksToFreeSpace 方法将之前放入内存 block 释放了 acquireUnrollMemory 申请 Unroll 内存很简单，就是调用 acquireStorageMemory 方法即可 acquireExecutionMemory 申请执行内存比较复杂 这里解释一下 maybeGrowExecutionPool maybeGrowExecutionPool 方法会收回之前借给 StorageMemroy 的空间，如果 StorageMemroy 剩余空间不够归还，就会令其释放之前加存储的 block，总而言之就是一定要归还。但是 executionMemory 借 storageMemory 的就不是强制归还。这是因为 executionMemroy 中存储的是运行数据，比如说 shuffle 数据，如果中途释放就会导致 task 失败，需要重新计算。 再看下调用 ExecutionMemroyPool#acquireMemory 方法 可以看到 executionMemoryPool 分配内存是一个死循环，不断调用之前的 maybeGrowExecutionPool，来检查是否需要从StorageMemoryPool回收空间。 如果实际分配的要比申请的数量少或者该 task 拥有的执行内存比 （1 / 2N）* executionMemoryPool 小（N 表示当前活动Task数）就会阻塞在这里，直到有其他Task释放内存为止，再进入下一次循环，直到申请到足够的内存。"},{"title":"Spark 存储系统5—MemoryStore","date":"2021-12-01T12:27:00.000Z","url":"/2021/12/01/Spark%20%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F5%E2%80%94MemoryStore/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"在 Spark 内存管理中提到 MemoryStore 是负责具体内存存取，现在了解一下 MemoryStore 的具体实现 先来看下数据在内存中是以什么形式存储的 Spark 的存储等级中有带 SER 和不带 SER 即存储序列化和非序列化两种，序列化方式存储为字节数组，而非序列化方式存储为对象，为此 MemoryEntry 有两个实现类 DeserializedMemoryEntry 和 SerializedMemoryEntry 可以看到非序列化也就是对象方式存储只能存储在堆内。 ok，了解完存储形式后，再来看下 MemoryStore，按照管理，先看构造函数和成员变量 由上可见，定义了一个 LinkedHashMap 用于实际存储数据，还定义了一些 Map 来保存 unroll 的堆内外内存使用信息。 数据写入 写入字节 写入字节的流程： 从 UnifiedMemoryManager 中确认 Storage 内存够不够，不够就从 Execution 内存中借，要是还不够就返回 false 调用 _bytes 函数，将数据转换成 ChunkedByteBuffer 封装成 SerializedMemoryEntry 直接写入 entries 写入 Iterator 数据 因为有时单个块对应的数据可能过大，不能一次性存入内存。为了避免造成OOM，就可以一边遍历迭代器，一边周期性地写内存，并检查内存是否够用，就像翻书一样。“展开”（Unroll）这个词形象地说明了该过程。有两个方法 putIteratorAsValues 、putIteratorAsBytes 分别是存储的对象和序列化后的数据，不过都是通过调用 putIterator 实现。 总结一下流程： 初始化需要的内存（这里不是申请内存，而是从可用内存的 book 中占用一部分） 不断的迭代，添加到 ValuesHolder，达到阈值时进行内存预估，不够了就申请 将数据封装成 MemoryEntry，写入到 LinkedListHashMap 中 返回 Right，这里返回的是 scala 的 Either 类型，它在Scala中表示不相交的两个结果的集合，即可能返回错误的结果（Left），或者正确的结果（Right） 再来看下申请和释放 Unroll 内存 申请 Unroll 内存 释放 Unroll 内存 数据读取 读取字节数据 因为存储为字节的数据都是序列化后的，所以读取出来只能是 SerializedMemoryEntry 读取对象数据 同上 淘汰缓存的数据 循环遍历 entries 映射中的块，找出其中能够被淘汰的块。所谓能够被淘汰，是指 MemoryMode 相同（即堆内对堆内，堆外对堆外，不能交叉），并且块ID对应的块数据不属于RDD。 为这些块加写锁，保证当前正在被读取的块不会被淘汰掉。记录将要被淘汰的块ID。 如果腾出的空间已经达到了目标值，就调用嵌套定义的 dropBlock() 方法真正地移除这些块，最终仍然调用了 BlockManager.dropFromMemory() 方法。该方法会产生两种结果：一是块仍然存在，只是 StorageLevel 发生变化（比如转存到了磁盘），就只需解开它的写锁；二是块被彻底地移除，就得调用BlockInfoManager.remove() 方法删掉它。最后将剩余未处理的块解锁。 如果腾出的空间最终仍然不能达到目标值，就不会执行淘汰动作，新的块也不会被存入。 在 putIterator 中，不断将数据插入到 ValueHolder 中，ValuesHolder 有两个实现类，DeserializedValuesHolder 和 SerializedValuesHolder DeserializedValuesHolder DeserializedValuesHolder 中有两个属性，vector 是一个 SizeTrackingVector，arrayValues 是一个数组 SizeTracker上面提到的SizeTrackingVector继承了这个特质，除了这个特质，还集成了PrimitiveVector类，但是PrimitiveVector类基本上就是对一个数组的简单包装。SizeTrackingVector最重要的功能：追踪对象的大小，就是在SizeTracker特之中实现的。 我大致说一下这个特质是如何实现对象大小跟踪和估算的，代码实现也并不复杂，感兴趣的可以看一看，限于篇幅这里就不贴了。 每插入一定数量的数据（姑且称之为周期），就会对当前的对象进行一次取样，而这个取样的周期会越来越长，以1.1倍的速率增长； 取样就是计算对象大小，并与前一次取样作比较，而且只会保留最近两次的取样数据； 每次取样其实就是获取两个数据，当前对象大小，当前插入的数据条数； 这样与上一次取样一比较，就能够计算出每条数据的大小了； 最后，在返回整个对象大小时，是拿最近一次取样时记录下的对象大小，以及根据最近的情况估算的每条数据的大小乘以自从上次取样以来新插入的数据量，二者相加作为对象大小的估算值， 可见这么做并不是什么精确，但是由于是抽样，而且抽样周期越往后面越长，所以对于数据插入的效率影响很小，而且这种不精确性其实在后续的内存检查过程中是有考虑到的。在所有数据插入完的收尾工作中，会对对象大小做一次精确计算。 SerializedValuesHolder 对于直接内存分配，spark并没有使用jdk的高级api，而是反射配合unsafe类分配直接内存，这样可以绕过jvm参数 MaxDirectMemorySize 的限制，这也体现了spark的作者尽可能的降低用户使用难度 另外，我们看到序列化流其实经过了层层包装（典型的装饰器模式），序列化和压缩以及分块是比较重要的几个点，感兴趣的话可以深究，序列化和压缩如果深入了解都是很大的课题，所以这里也仅仅是蜻蜓点水，不深究了。 总结MemoryStore.scala 这个文件中乍看代码有八九百行，但是其实很大部分代码是一些辅助类，比较核心的写入逻辑也就是前面提到的几个方法，再加上核心的两个类 DeserializedValuesHolder 和 SerializedValuesHolder 实现了以对象或字节数组的形式存储数据。"},{"title":"Spark 存储系统6—DiskBlockManager、DiskStore","date":"2021-12-01T12:27:00.000Z","url":"/2021/12/01/Spark%20%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F6%E2%80%94DiskBlockManager%E3%80%81DiskStore/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"关于内存的块管理已经差不多了，接下来看下基于硬盘的块管理。与内存管理的 MemoryManager 类似，硬盘管理有一个 DiskBlockManager。按照惯例，先看构造方法和属性 看下是如何找到一个文件的 根据 block 的命名取 hash hash 对父文件夹数量取余，确定父文件 对父文件夹数量取商，再对子文件数量（默认 64）取余确定子文件 如果文件已经存在则返回文件，如果不存在则创建子目录并格式化为两位的十六进制表示 除了获取单个文件，还可以获取所有文件和块 id 的 getAllFiles、getAllBlocks 只是简单的遍历即可。 创建临时文件 这两个方法都很简单，用于创建 Spark 运行过程中的中间文件和 shuffle write 输出文件，分别用 TempLocalBlockId 和 TempShuffleBlockId 来表示。 关闭钩子与关闭 如果 deleteFilesOnStop 构造方法参数为 true 则在关闭前会删除本地存储的文件和目录。 与内存写入一样，磁盘写入也由 DiskStore 负责。先看下构造方法和属性 写入块 先判断这个块是否已经写入了 contains 方法会调用 DiskBlockManager.getFile 方法。 调用 DiskBlockManager.getFile()方法打开块ID对应的文件 获取该文件的 WritableChannel，（CountingWritableChannel 继承自 WritableChannel，添加了统计字节数功能），（NIO中的写通道，表示可以通过调用write()方法向文件写入数据） 调用参数中传入的 writeFunc 函数进行写入 将块大小更新到 blockSizes 中 写入字节 传入的参数除了块id 以外，还需要传入封装成 ChunkedByteBuffer 的对象，然后调用 put 方法，writeFunc 函数为 ChunkedByteBuffer.writeFully，负责将数据以一定的 Chunk 大小分块写入 WritableByteChannel。 读取字节 在加密环境下和非加密环境下返回的结果是不同的，前者是 EncryptedBlockData 对象，后者是 DiskBlockData 对象，而它们都是 BlockData 的子类。顾名思义，BlockData 就是对磁盘块数据的具体封装，下面选择最常见的 DiskBlockData 来看一看。 BlockData 特征只是定义了块数据的转化方式，具体的细节则留给各个实现类。我们具体看看 toChunkedByteBuffer() 和 toByteBuffer() 这两个方法。toChunkedByteBuffer() 方法会将文件转化为输入流 FileInputStream，并获取其 ReadableFileChannel，再调用 JavaUtils.readFully() 方法将从 Channel 中取得的数据填充到 ByteBuffer 中。每个 ByteBuffer 即为一个 Chunk，所有 Chunk 的数组形成最终的 ChunkedByteBuffer toByteBuffer() 方法会检查块大小是否小于 spark.storage.memoryMapThreshold（终于出现了）。如果小于的话，就会采用与 toChunkedByteBuffer() 相同的方式直接填充 ByteBuffer。反之，就调用 ReadableFileChannel.map() 方法将数据映射到 MappedByteBuffer 中，即进程的虚拟内存中。不过，考虑到内存映射的应用场景的话，2MB的阈值可能有点小（保守）了 参考Spark Core源码精读计划#27：磁盘块管理器DiskBlockManagerSpark Core源码精读计划#28：磁盘存储DiskStore 待学习ChunkedByteBuffer"},{"title":"Spark 存储系统7—BlockManager","date":"2021-12-01T12:27:00.000Z","url":"/2021/12/01/Spark%20%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F7%E2%80%94BlockManager/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"终于到 BlockManager 了，按照惯例，先看构造方法 BlockManager 的初始化，在 SparkEnv 中调用 BlockManager#initialize 方法进行初始化 初始化步骤： 初始化 BlockTransferService 和 BlockStoreClient 初始化 blockReplicationPolicy 根据 ExecutorId 创建 BlockManagerId，并调用 BlockManagerMaster.registerBlockManager() 方法注册此 ID 与从 RPC 端点。注册成功后，BlockManagerMaster 会返回另一个正式的 ID 生成 Shuffle 服务的 ID。如果当前节点是 Executor 并启用了外部 Shuffle 服务的话，就调用 registerWithExternalShuffleServer() 方法注册外部 Shuffle 服务 创建 HostLocalDirManager 接下来将从块的读与写两个方面展开。 块的读取： 在BlockManager中提供了多种对块进行读写的方法，其中一个将读写进行统一的入口是getOrElseUpdate()方法。因为块可以由RDD物化而来，因此我们可以方便地在RDD类中（具体来说是RDD.getOrCompute()方法）找到对它的调用。为了方便分析，本文就由它来入手。先顺便看一下源码吧。 首先会根据 blockId 从本地或者远程拉取 Blcok 数据，如果本地或者远程都没有，则会根据 makeIterator 函数将数据转换成迭代器并写入，最终封装成 BlockResult 返回，写入部分我们放到后面看，先看看是怎么从本地或者远程拉取数据的。 从上面可以看到，先尝试从本地拉取数据，如果本地没有则会从远程拉取 大致流程： 根据 blockId 加读锁，并尝试返回对应块元数据 BlockInfo 如果没有加上读锁或者没有返回 BlockInfo 表示该块没有存储在本地，否则根据 StorageLevel 判断按内存、磁盘的优先级进行数据读取 如果 StorageLevel 使用到了内存，并且 MemoryStore 中存储了数据，根据是否序列化调用 getValues 或者 getBytes，获取块数据的迭代器表示，封装到 BlockResult 中返回，并释放锁。 如果内存中没有数据，并且 StorageLevel 使用到了磁盘，且 DiskStore 中存储了数据，调用 diskStore.getBytes 获取磁盘中的数据流，然后根据是否序列化做不同处理。其中还会调用 maybeCacheDiskValuesInMemory 或 maybeCacheDiskBytesInMemory 尝试将数据缓存到内存中，以加快速度。获取块数据的迭代器表示，封装到 BlockResult 中返回，并释放锁。 如果从内存和磁盘都读取失败，调用 handleLocalReadFailure 方法处理本地读取错误 从远程读取数据 这个方法主要是调用 getRemoteBlock 返回远程数据。因为远程数据必须要通过序列化才能传输，从远程先获取字节流，数据到本地后先进行反序列化。 大致流程： 向 driver 询问 block 的位置信息 根据 BlockLocationsAndStatus 获取 block 大小 如果数据是在本机（一台主机上有多个 Executor，可能其他 Executor 有这份数据），尝试从本机获取 ManagedBuffer，并进行反序列化 如果数据不在本机，则从远程拉取 先尝试获取本地文件 根据是否加密进行不同处理 如果是不加密数据，封装成 FileSegmentManagedBuffer 进行读取 大致流程： 先对 block 位置按照，本机、本机架、其它进行排序 根据远近顺序获取数据 如果失败次数大于 block 位置数，直接返回 None 第 3 条有时候会导致大量重试，所以设定一个阈值，刷新块位置 接上之前的写入流程，在 getOrElseUpdate 中会调用 duPutiterator 进行写入 可以看到 doPutIterator 方法的本质其实是调用 doPut 方法，整个写入的逻辑作为一个函数，作为 doPut 方法的柯里化参数。数据的写入与数据的读取流程差不多，只不过读取变成了写入，大致流程： 判断 StorageLevel 是否使用内存，如果是，判断是否存储为对象 调用 memoryStore.putIteratorAsValues 进行写入，如果写入失败，再判断是否使用硬盘，如果是将数据序列化写入硬盘，如果没使用硬盘，返回写入成功部分。 如果存储为序列化数据，调用 memoryStore.putIteratorAsBytes 进行写入，后续流程与上述差不多 如果不使用内存，只使用硬盘，将数据序列化写入硬盘 判断存储状态，验证是否符合 StorageLevel 要求 更新 Metrics 如果副本数大于 1，从本地取出序列化后的数据，复制副本 再来看下 doPut 方法 doPut 挺简单的，大致流程： 尝试加新块写入锁，如果已经有读锁在，表示 block 已经存在，不需要再写了。 执行 doPutIterator 函数流程，进行写入 如果写入失败，删除已经写入的 最后再来看下如何复制副本 大致流程： 通过 peersReplicatedTo、peersFailedToReplicateTo 记录已经成功复制副本和失败的 BlockManager 从缓存获取可以保存副本的 BlockManager 通过 maxReplicationFailureCount 最大失败次数控制重试次数，这里最大失败次数为 1，表示可以重试一次 将数据封装成 BlockManagerManagedBuffer，通过 BlockTransferService 进行传输复制 如果遇到异常，重新获取其它 blockManager（第二次会向 BlockManagerMaster 拉取，并更新缓存）, 过滤掉已经保存了副本的和复制副本失败的 如果最终复制的副本数小于 StorageLevle 要求的副本数，返回 false，失败 参考资料： Spark Core源码精读计划#31：BlockManager块写入流程 待学习ExternalBlockStoreClientNettyBlockTransferService是如何获取远程数据的"},{"title":"Spark 缓存","date":"2021-11-29T12:27:00.000Z","url":"/2021/11/29/Spark%20%E7%BC%93%E5%AD%98/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"Spark 缓存 Spark 有个特点叫惰性计算，也就是只有碰到 action 算子才会触发计算。如果 RDD 的 transform 算子过多时，可以通过缓存的方式加快运算速度，例如 上面这个例子中，首先读入 readme.md 文件，并进行一系列的 transform 操作，直到碰到 foreach 和 count 两个 action 算子才触发计算，但是由于中间没有进行缓存，两个 action 算子都要从头进行计算。 在 transform 算子链的最后加上 cache 或者 persist 算子对之前的 RDD 进行缓存。然后在第一碰到 action 算子进行计算时就会将前面 RDD 的结果缓存起来，第二次碰到 action 时就可以直接使用了，不必在从头开始算了。 我们在使用 Spark 时对数据进行持久化（或者称为缓存）时经常使用 cache 或 persist 算子，那么这两个算子的区别在哪呢？ persist vs cache首先来看下 cache，在 RDD#cache 中，cache 方法直接调用了 persist 好吧，那就看看 persist persist 有两个个方法，分别是默认存储级别，可选存储级别，之后会讲到什么是存储级别，现在先把它理解为数据存在内存中还是磁盘中。 由上面可以知道 cache 算子其实就是存储级别为 MEMORY_ONLY 的 persist 算子，所以如果不指定存储级别调用 cache 和 persist 效果是一样的。 接着看 persist 可以看到，persist 算子并没将数据缓存到内存或这磁盘，只是简单的记录了一下需要缓存的 RDD，因为 persist 算子并不是 action 算子，真正的缓存需要第一次碰到 action 算子才会触发。 在 RDD 类中定义了一个迭代器，RDD#iterator，用于迭代访问数据 在访问数据时就会判断 storageLevel 是否修改过（是否有调用过 cache 或 persist 算子），如果 storageLevel 修改过则会进入 getOrCompute 方法 然后根据 blockManager 获取或者更新数据 至此，持久化就已经完成了，后续如果有 action 算子使用这个 RDD 则直接从内存或者磁盘中获取，不需要重新计算了。 删除数据Spark 自动监控各个节点上的缓存使用率，并以最近最少使用的方式（LRU）将旧数据块移除内存。如果想手动移除一个 RDD，而不是等待该 RDD 被 Spark 自动移除，可以使用 RDD.unpersist() 方法 存储级别Spark 的存储级别有以下几种 这几个存储级别是通过以下几个参数组合得到的 解释一下这几种存储级别，其中 DISK_ONLY、MEMORY_ONLY、MEMORY_AND_DISK 都好理解，就是只使用磁盘、只使用内存、使用内存和磁盘。 DISK_ONLY_2、MEMORY_ONLY_2 这种后缀有个 2 的表示 replication 为 2，即两份副本 MEMORY_ONLY_SER、MEMORY_AND_DISK_SER 这种后缀带 SER 表示以序列化的 Java 对象的形式进行存储，这种存储比较省空间，但是读取时需要反序列化，会损耗一些 CPU 计算时间 最后提一下 cache/persist 与 checkpoint 的区别： cache/persist 是将数据写到本机内存或磁盘中，随着作业运行失败或执行完成，数据将被清除。 checkpoint 一般是将数据写到远程文件系统，作业运行失败或完成数据都可以保存。cache/persist 只是将数据保存起来，并不会破坏 lineage。checkpoint 会切断 lineage，在第一个 Action 算子被调用时触发，且在这个 Job 执行完成后，才会从后往前回溯找到标记了 checkpoint 的 RDD，然后新启动一个 Job 执行 checkpoint 任务。建议对checkpoint 的 RDD 使用 Cache 缓存，这样 checkpoint 的 job 只需从 Cache 缓存中读取数据即可，否则需要再从头计算一次 RDD。 参考资料Spark 持久化（cache和persist的区别） RDD的cache和persist原理"},{"title":"Spark 统一内存管理器","date":"2021-09-05T07:44:36.000Z","url":"/2021/09/05/Spark%20%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%99%A8/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"],["Spark 内存管理","/tags/Spark-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"]],"categories":[[" ",""]],"content":"Spark 内存按 JVM 堆划分可以分为堆内内存和堆外内存，如果按存储内容划分，可以分为存储内存（Storage）和执行内存（Execution）。 堆内内存堆内内存由 JVM 统一管理，大小在 Executor 启动时 –executor-memory 或 spark.executor.memory 参数进行配置配置，而且一个 Executor 中的 Task 共享 Executor 的内存。任务在缓存 RDD 数据和广播（Broadcast）数据时占用的内存被规划为存储（Storage）内存，而这些任务在执行 Shuffle 时占用的内存被规划为执行（Execution）内存，剩余的部分不做特殊规划，那些 Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间。不同的管理模式下，这三部分占用的空间大小各不相同 堆外内存为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。堆外内存可以被精确地申请和释放，而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。通过 spark.memory.offHeap.enabled 参数启用堆外内存，由 spark.memory.offHeap.size 配置可用堆外内存大小。堆外内存与堆内内存的划分差不多，不过只有存储内存和执行内存。 内存管理方式Spark 现在（3.0.0）的内存管理方式为统一内存管理（Unified Memory Manager），以前还有一个静态内存管理（Static Memory Manager），现在已经弃用了，静态内存管理主要是存储内存和执行内存的大小是固定的。而统一内存管理是存储内存和执行内存共享一块空间，可以动态占用对方的空间。 图片来自Spark内存管理详解 图片来自Spark内存管理详解 其中最重要的优化在于动态占用机制，其规则如下： 设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围 双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block） 执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间 存储内存的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂 图片来自Spark内存管理详解 多任务内存分配 Executor 内运行的任务同样共享执行内存，Spark 用一个 HashMap 结构保存了任务到内存耗费的映射。每个任务可占用的执行内存大小的范围为 1/2N ~ 1/N，其中 N 为当前 Executor 内正在运行的任务的个数。每个任务在启动之时，要向 MemoryManager 请求申请最少为 1/2N 的执行内存，如果不能被满足要求则该任务被阻塞，直到有其他任务释放了足够的执行内存，该任务才可以被唤醒。 参考资料 Spark内存管理详解"},{"title":"UML 类图","date":"2021-08-18T03:16:35.000Z","url":"/2021/08/18/UML%E7%B1%BB%E5%9B%BE/","tags":[["UML","/tags/UML/"]],"categories":[[" ",""]],"content":" 实体表示 访问修饰符： +：public -：private #：protected 不带符号：default 接口 接口 UML 在接口名上方有 &lt;&lt;Interface&gt;&gt; 标识 抽象类 抽象类的类名和抽象方法使用斜体表示 实现类 关系表示 实现关系 实现关系用虚线空心箭头表示 泛化关系 泛化关系是 is a 的关系，即继承关系，使用实线空心箭头表示 关联关系 关联关系的实现形式为一个对象持有另一个对象的引用。UML 中采用带方向箭头实线表示， 依赖关系 依赖关系是一种弱关联关系，表示 A use a B 即 A 使用 B，Java 中的表现形式一般为，构造器或方法中的局部变量，或者是构造器或方法的参数或者方法的返回值，或者 A 调用 B 的静态方法 在 UML 使用虚线带方向的箭头表示 聚合关系 聚合关系是关联关系中的一种特例，体现整体与部分的关系，has a 的关系，这种整体和部分是可以分离的，部分可以属于多个整体，比如公司部门和员工的关系，一个员工可以属于多个部门，所以聚合关系也叫共享关系，UML 采用空心菱形加实线箭头表示，箭头指向部分，菱形指向整体 组合关系 组合关系也是一种关联关系的特例，体现整体与部分的包含关系，即 contains a 的关系，这种关系整体与部分是不可分的，部分不能给多个整体共享，作为整体的对象负责部分的对象的生命周期，关系比聚合更强，所以也叫强聚合。比如，人包含各种器官的关系。UML 使用实心菱形加实线箭头表示，箭头指向部分，菱形指向整体 在Java代码形式上，聚合和组合关系中的部分对象是整体对象的一个成员变量。但是，在实际应用开发时，两个对象之间的关系到底是聚合还是组合，有时候很难区别。在Java中，仅从类代码本身是区分不了聚合和组合的。如果一定要区分，那么如果在删除整体对象的时候，必须删掉部分对象，那么就是组合关系，否则可能就是聚合关系。从业务角度上来看，如果作为整体的对象必须要部分对象的参与，才能完成自己的职责，那么二者之间就是组合关系，否则就是聚合关系。 例如，汽车与轮胎，汽车作为整体，轮胎作为部分。如果用在二手车销售业务环境下，二者之间就是聚合关系。因为轮胎作为汽车的一个组成部分，它和汽车可以分别生产以后装配起来使用，但汽车可以换新轮胎，轮胎也可以卸下来给其它汽车使用。如果用在驾驶系统业务环境上，汽车如果没有轮胎，就无法完成行驶任务，二者之间就是一个组合关系。再比如网上书店业务中的订单和订单项之间的关系，如果订单没有订单项，也就无法完成订单的业务，所以二者之间是组合关系。而购物车和商品之间的关系，因为商品的生命周期并不被购物车控制，商品可以被多个购物车共享，因此，二者之间是聚合关系。 参考资料 30分钟学会UML类图 "},{"title":"Spark 调度策略","date":"2021-08-09T12:56:50.000Z","url":"/2021/08/09/Spark%20%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"概览Spark 的调度有两层，一是 Spark 应用间的调度，即不同 SparkContext 间的调度，这层的调度主要由 YARN 负责，就不过多赘述；二是 Spark 应用内 Task 级别的调度，即相同 SparkContext 间的调度，主要分析的重点。 Spark 应用内的调度有两种模式：FIFO(先进先出队列)、FAIR(公平队列) FIFO FAIR 默认使用 FIFO 调度，所以，如果想要使用 FAIR 调度，需要通过 spark.scheduler.mode 参数进行显示指定 或者提交任务时指定。 Spark 在初始化 TaskScheduler 时就会根据调度模式创建不同的 ScheduleBuilder FIFO因为 FIFOSchedulableBuilder 只有一个队列，所以的实现比较简单，添加 TaskSetManager 只要直接添加即可 FIFO 调度算法 FIFO 调度主要是按照先进先出的顺序进行调度，流程为： 比较 jobId，上面代码中的 priority，哪个 jobId 小，就先调度哪个，因为 jobId、stageId 都是递增的，小的肯定是先进入队列的。 如果 jobId 相同，则比较 stageId，哪个 stageId 小，就先调度哪个 FAIRFairSchedulableBuilder 的实现就比较复杂，根据上面 FAIR 的图可以看出，FAIR 调度方式有多个子队列。先来看看 FAIR 调度算法的两个关键参数： weight：当前资源池相对于其他的资源池可以分配到的资源，默认都为 1。如果当前资源池的 weight 设置为 2，其他资源池为 1，那么当前资源池可以分配到的资源就是其他资源池的 2 倍。 minShare：每个资源池最小资源分配数（CPU核数），默认为 0。这个最小不是指初始化时就分配，而是在资源分配时优先满足未到达 minShare 的资源池，当所有 minShare 都达到后，再按照 weight 进行分配，这样能确保每个资源池都能获得一定量的资源池。 weight 和 minShare 通过在 fairscheduler.xml 中进行配置 fairscheduler.xml 中的 schedulingMode 是指子队列的调度模式，不是 rootPool 的调度模式。 FairSchedulableBuilder 会根据 fairscheduler.xml 中配置的子队列初始化时加入到 rootPool 中。 初始化 Fair Pool Fair Pool 添加 TaskSetManager在添加 TaskSetManager 时会先判断是否传入子队列的名称，如果没有则使用默认队列（default） FAIR 调度算法FAIR 调度算法要稍微复杂一点，说明一下 runningTasks 表示已经在使用的资源数。 下面是 FAIR 调度算法代码： FAIR 判断的流程如下： 判断 minShare 是否都满足，优先分配给未达到 minShare 的资源池 如果 minShare 都未满足，按照 runningTasks / minShare 进行比较，哪个资源池比例小就分配给哪个资源池。 如果 minShare 都满足，按照 runnigTasks / weight 进行比较，哪个资源池比例小就分配给哪个资源池。 如果以上几种情况大小都相等，则按资源池名称字典序排序。 任务出队 schedulableQueue 是 Pool 的存储队列，由 ConcurrentLinkedQueue 实现，还用 ConcurrentHashMap 维护了一个 schedulableNameToSchedulable 的映射 "},{"title":"策略模式","date":"2021-07-27T06:47:00.000Z","url":"/2021/07/27/%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/","tags":[["设计模式","/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"],["策略模式","/tags/%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/"]],"categories":[[" ",""]],"content":" 策略模式属于对象的行为模式。其用意是针对一组算法，将每一个算法封装到具有共同接口的独立的类中，从而使得它们可以相互替换。策略模式使得算法可以在不影响到客户端的情况下发生变化–《Java 与模式》 策略模式主要有三个角色： Context 环境对象角色 Strategy 策略抽象类或接口角色 ConcreteStrategy 具体策略角色 UML 类图如下 图片来自：策略模式 环境对象持有策略抽象类或接口，策略抽象类定义具体策略类需要实现的公共抽象方法或接口 伪代码如下： 实际使用中可以不用 Client 这角色，可以在 Context 中根据条件选择具体的策略 策略模式的优点： 可以不用修改 Context 添加新的策略，只需要继承或实现策略抽象类或策略接口即可 将不同的策略封装到一个单独的类中，将策略的使用与策略的实现分离解耦策略模式的缺点 Client 需要知道所有的策略，因为 Client 需要选择具体的策略，所以必须知道所有的策略才行 将每个策略都封装到一个单独类中，如果策略过多，对应的类也就多了 "},{"title":"Spark Join","date":"2021-07-06T12:10:20.000Z","url":"/2021/07/06/Spark-Join/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"],["Spark SQL","/tags/Spark-SQL/"]],"categories":[[" ",""]],"content":"Join 是 SQL 中非常重要的语法，只要稍微复杂一点的场景就离不开 Join，甚至是多表 Join，如下图，Join 有三个关注点：Join 方式、Join 条件、过滤条件 图片引用自Spark SQL 之 Join 实现 Spark Join 实现方式在聊实现方式前，需要了解什么 Hash Join，hash join 是指将一个表根据 Join Key 做 hash（暂且称为查找表），然后遍历另一个表（暂且称为遍历表），遍历时在 hash 表中根据 join key 查找，这样时间复杂度就是 O(M(遍历一个表) + 1(hash 查找))，入下图，Build Table 为查找表，Probe Table 为遍历表 图片引用自Spark难点 | Join的实现原理 Broadcast Hash Joinok，对 hash join 了解后，再来聊聊 Broadcast Hash Join。在 join 过程中有这么一种情况——一张很大的表 join 一张很小的表（一般是维表），这时如果对大表做 shuffle 代价可能比较大，Spark 会对这种情况进行优化，将小表作为广播变量广播到各个节点中，然后在节点中做 hash join。这样对一个大表的 shuffle 就变成对一个小表的广播，而 Spark 广播变量使用 BitTorrent 进行优化(感兴趣可以点这Spark Accumulator &amp; Broadcast)，性能比 Shuffle 有很大的提省，只要每个节点的大表分区做一个 map 就可以完成，这种方式也被称为 map join。 图片引用自Spark Join Strategies — How & What? 虽然 Broadcast Hash Join 的方式快，但是随着广播的表大小增长，性能也逐渐下降，所以 Spark 只会将小于 spark.sql.autoBroadcastJoinThreshold (默认 10M)的表采用 Broadcast Hash Join，spark.sql.autoBroadcastJoinThreshold 参数设置为 -1，可以关闭 BHJ，实现可见 org.apache.spark.sql.execution.joins.BroadcastHashJoinExec 特点： 只能用于等值连接 除了 full outer join，支持所有 join 广播表大小 &lt; spark.sql.autoBroadcastJoinThreshold (default 10M) Shuffle Hash Join当 Broadcast Table 大到一定程度后，将整个表广播已经不太划算了，还不如 shuffle。这就会用到 Shuffle Hash Join，Spark 会根据 Join Key 进行 shuffle，那么相同的 key 一定都在同一个节点中，再根据 Hash Join 的方式进行单机 Join 图片引用自Spark Join Strategies — How & What? Shuffle Hash Join 需要在内存中建立 hash table 所以有 OOM 风险，使用 Shuffle Hash Join 的前提为 spark.sql.join.preferSortMergeJoin 必须为 false，实现见 org.apache.spark.sql.execution.joins.ShuffledHashJoinExec 特点： 只支持等值连接 除了 full outer join，支持所有 join spark.sql.join.preferSortMergeJoin 必须为 false 小表的大小 &lt; spark.sql.autoBroadcastJoinThreshold(default 10M) * spark.sql.shuffle.partitions(default 200) 小表的大小 * 3 &lt;= 大表大小 Sort Merge Join既然 Shuffle 不可避免，那么有没有其他优化的方式呢？Spark Shuffle 对排序有着很好的支持，所以在 Shuffle Write完之后，两个表都是局部有序的，那么可不可以在 Shuffle Read 阶段就完成 Join。由于两个表根据 Join Key 分区数据都是有序的，那么在 Shuffle Read 时，可以根据采用 Hash Join 的思想，只不过这次的查找表不是 hash 查找，而是顺序查找。对遍历表一条一条遍历，对查找表顺序查找，下一条数据只要从当前位置查找即可，不需要从头开始查找。当 Shuffle 完成时，Join 也就完成了。 图片引用自Spark Join Strategies — How & What? 显然，如果要使用 Sort Merge Join ，Join Key 必须是可排序的，实现可见 org.apache.spark.sql.execution.joins.SortMergeJoinExec 特点： 只支持等值连接 支持所有类型的 join Join key 必须可排序 Broadcast nested loop join 如上面代码所示，Broadcast nested loop join 通过循环嵌套的方式进行 join，效率非常低。具体实现见 org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec 特点： 支持等值和不等值连接 支持所有类型的 join Cartesian product join(Shuffle-and-replicate nested loop join)笛卡尔积 join，与普通 SQL 一样，如果 join 时不加连接条件就会产生笛卡尔积连接。具体实现可以看看 org.apache.spark.sql.execution.joins.CartesianProductExec 特点： 支持等值和不等值连接 只支持 inner join 需要开启 spark.conf.set(&quot;spark.sql.crossJoin.enabled&quot;, &quot;true&quot;) Join 方式选择既然有这么多种 Join 方式，那么 Spark 是怎么选择合适的 Join 方式呢？ Spark 根据等值和非等值连接进行划分 等值连接 其中用户选择遵循下方代码顺序 即 Broadcast Hash Join, Sort Merge Join, Shuffle Hash Join, Cartesian Product Join, Broadcast Nested Loop Join 的顺序 非等值连接 用户选择顺序为： 即 Broadcast Nested Loop Join, Cartesian Product Join。上图有两次 Broadcast Nested Loop Join，是因为第一次 Broadcast Nested Loop Join 会先尝试是否能广播左表或者右表，如果都不能则选择 Cartesian Product Join，最后再用 Broadcast Nested Loop Join 兜底 Spark 中特殊的 Joinleft semi joinleft semi join 是以左表为准，如果查找成功就返回左表的记录，如果查找失败则返回 null，如下图所示。 图片引用自Spark SQL 之 Join 实现 left anti joinleft anti join 则是与 left semi join 相反，也是以左表为准，如果查找成功就返回 null，如果查找失败则返回左表记录，如下图 图片引用自Spark SQL 之 Join 实现 我不知道这两种特殊的 Join 方式是不是 Spark 特有的，但是是我学习 Spark 之后才知道有这两种 Join 方式 参考资料Spark Join Strategies — How &amp; What? 每个 Spark 工程师都应该知道的五种 Join 策略 Spark SQL 之 Join 实现"},{"title":"Spark Accumulator & Broadcast","date":"2021-06-24T04:29:44.000Z","url":"/2021/06/24/Spark-Accumulator-Broadcast/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"Spark 中有两类共享变量，即累加器（Accumulator）和广播变量（Broadcast） Accumulator累加器，顾名思义，是用于累加计数的，那么为什么需要累加器呢，普通变量不能计数吗？先看一段代码 这段代码的输出结果为 0，为什么呢？这里涉及到一个 Spark 闭包问题，所谓的闭包可以理解为 在函数作用域中修改作用域外的变量。在计算前 Spark 会将闭包序列化并发送到 executor 上，上面代码中会将 counter 序列化打包，然后在 executor 中反序列化，这时 Driver 端和每个 Executor 中都有一个 counter，但是 Executor 中的 counter Driver 端的副本，也就是说如果在 Executor 中修改了 counter 是不会影响 Driver 端 counter 的值，所以没法起到计数的效果。所以计数就需要用到 Accumulator 从 LongAccumulator 代码看来 _sum 也只是普通的变量，那为什么 LongAccumulator 中的变量就能起到计数的效果呢在 LongAccumulator 里有一个 merge 方法，这个 merge 方法在 task 运行完成后被调用，会将不同 Executor 的变量合并到 Driver 上。 需要注意的是 Accumulator 的值只能在 Driver 端查看。 上图左边为使用普通变量，右图为使用 Spark Accumulator，区别就在于 Spark Accumulator 在 Task 任务执行后将变量回传给 Driver 进行累加。 Accumulator 使用Spark 内置有三种 Accumulator，分别是 LongAccumulator、DoubleAccumulator 和 CollectionAccumulator，分别对应 Long、Double、List 三种计数类型。在使用 Accumulator 时需要 action 算子触发计算。 SparkContext 集成了三种 Accumulator，在使用时只需要传入一个 Accumulator name 即可，在 Spark Web UI 中的 Stages 页面可以查看声明的 Accumulator 自定义 Accumulator自定义 Accumulatro 只需要继承 AccumulatorV2 即可 完成定义后，在使用时将 Accumulator 在 SparkContext 中注册即可 运行上面的代码发现每次运行的结果都不一样 这是因为 merge 的时候顺序是不确定的，哪个task 先执行完就先 merge 哪个，所以在自定义 Accumulator 的时候一定要注意合并顺序不能影响最终结果，这样才算是正确的 Accumulator Accumulator 注意事项下面这段代码，counter 第一次打印 15，第二次打印 30 因为调用了两次 action 算子，这段代码会被构建成两个 job，每个 job 是从 DAG 最开始进行计算，都会对 counter 进行操作，所以第二次会打印 30 我们只需要将中间结果 cache 隔断之前的血缘即可 ，就能解决这个问题 这样两次打印的值就一样了。 Accumulator 小技巧Accumulator 不仅可以作为累加器，还能作为“累减器“，只需要累加的数值改成负值即可，只适用于数值计算 BroadcastSpark 中另一个共享变量就是广播变量（broadcast）。一般一个 Executor 中会有多个 Task，如果不使用广播变量，Spark 需要将变量副本传输到每个 Task 中，造成带宽、内存和磁盘的浪费，降低执行效率，使用广播变量则只会传输一份副本到 Executor 中，Executor 中的 Task 共享这一份副本。假设有 100 个 Executor，每个 Executor 中有 10 个 Task，然后有一个 10M 大小的 List 需要考拷贝副本，如果不使用广播变量的情况需要拷贝 100 * 10 * 10 = 10000M，大概 10G；如果使用广播变量只需要 100 * 10 = 1000M，减小了 10 倍。所以广播变量是优化 Spark 任务的一个小技巧。 Broadcast 原理broadcast 传输文件采用 BitTorrent 协议，也就是常说的 BT 下载，和点对点（point-to-point）的协议程序不同，它是用户群对用户群（peer-to-peer），可以看下这个小游戏 BitTorrent，大概思想为把一个文件切分成多个块，每个下载的机器上都存放一块或多块，有新机器加入下载就可以到不同的机器上下载不同的块，降低 Server 的负载。 Spark Broadcast 的原理与 BitTorrent 类似，Spark 使用 blockManager 管理文件 block 写流程 将需要广播的变量切分成一个个 chunk (默认 4M） 计算这些 chunk 的 Adler-32 checksum 每个 chunk 加上一个 pieceId （ “broadcast_” + broadcastId + “_” + “piece” + number），这个 pieceId 作为 blockId 写入文件系统 读流程 随机获取 pieceId 根据 blockManager 获取 block 校验校验和 按照 pieceId 顺序拼接各个 block Broadcast 使用 Broadcast 只读问题为什么 Broadcast 只能读不能修改？因为广播变量会传输到多个 Executor 上，如果某个 Executor 能够修改了 Broadcast，那么 Spark 就要保证修改的 Broadcast 能及时同步到每台机器上，在同步的时候要保证数据的一致性以及同步失败怎么容错等等问题，所以 Spark 干脆就让 Broadcast 不可修改 参考资料Spark笔记之累加器（Accumulator） spark 广播变量的设计和实现"},{"title":"Spark Shuffle 源码浅析","date":"2021-06-20T06:26:00.000Z","url":"/2021/06/20/Spark%20Shuffle%20%E6%BA%90%E7%A0%81%E6%B5%85%E6%9E%90/","tags":[["源码","/tags/%E6%BA%90%E7%A0%81/"],["Spark","/tags/Spark/"],["Shuffle","/tags/Shuffle/"]],"categories":[[" ",""]],"content":"Spark Shuffle 的相关介绍可以看看(这篇)[]，此文主要是 Spark Shuffle 的源码浅析，比较枯燥。 入口 Spark Shuffle 源码的入口可以从 ShuffleMapTask#runTask 开始，以 dep.shuffleWriterProcessor.write(rdd, dep, mapId, context, partition) 中的 write 方法为入口。进入 write 方法后，可以看到从 shuffleManager 获取一个 ShuffleWriter，然后就调用 writer#write，进行写数据，写入完成返回写入 ShuffleWriter 这个 ShuffleWriter 是一个抽象类，有三个子类，分别是 BypassMergeSortShuffleWriter、SortShuffleWriter、UnsafeShuffleWriter， 这三个 Writer 是如何选择的呢？我们可以进入 ShufflerManager#getWriter 方法看看，由于 ShuffleManager 是一个接口，我们到它的实现类里看看，现在版本的 Spark，ShuffleManager 只有 SortShuffleManager 一个实现类，之前版本（1.6）还有一个实现类 HashShuffleManager。 可以看到三个 Writer 是根据不同的 Handle 进行选择的，而 Handle 是怎么确定的？ 可以看 SortShuffleManager#registerShuffle 方法 根据不同判断进而选择不同的 Handle，下表对不同情况进行了总结 Writer Handle 条件 BypassMergeSortShufflerWriter BypassMergeSortShuffleHandle 1. 不能开启 map 端预聚合2. 分区数 &lt;= SHUFFLE_SORT_BYPASS_MERGE_THRESHOLD(默认200) UnsageShuffleWriter SerializedShuffleHandle 1. 不能开启 map 端预聚合2.支持重定位序列化（Java 序列化不支持，需要使用 Kryo）3.分区数 &lt;= 2^24 16777216 SortShuffleWriter BaseShuffleHandle 上述两种的其它情况 写数据步骤 返回【1】中的 write，来看看三种 Writer 的不同写方式。先来看看 SortShuffleWriter，顾名思义，SortShuffleWriter 是会进行排序的，但是排序也是有条件的，即是否开启 map 端预聚合 可以看出写磁盘的操作都是由 ExternalSorter 完成的，重点看下 ExternalSorter 先看下 ExternalSorter#insertAll 还有一个重要呢的写操作和合并之前的溢写临时文件的方法 ExternalSorter#writePartitionedMapOutput 参考资料 Spark Shuffle Write 和Read "},{"title":"Spark repartition vs coalesce","date":"2021-06-15T10:00:00.000Z","url":"/2021/06/15/Spark%20repartition%20vs%20coalesce/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"在平时经常需要对 RDD 重新分区，或增加分区以提高并行度，或减少分区写文件以减少小文件数量，对 RDD 的分区数量操作有 repartition 和 coalesce 两个算子 repartitionrepartition 算子会返回一个指定分区数的新 RDD，可以用来增加或者减小分区数量以调整 RDD 的并行度。不过这种增加和减小分区数量是以 shuffle 为代价换来的。 repartition 的代码很简单，接受一个分区数参数，然后调用 coalesce 方法 coalescecoalesce 有两种模式，分别是 shuffle 和非 shuffle 模式，从 repartition 调用的是 shuffle 模式，这一点从参数可以看出来。 如果 shuffle 参数为 true 则会将一个 ShuffledRDD 封装进 CoalescedRDD。如果 shuffle 参数为 false(默认) 则创建一个 CoalescedRDD 对象。主要看看非 shuffle 模式 coalesce 的非 shuffle 模式只能用来减少分区，例如有 1000 个分区，可用用 coalesce(100) 减少至 100 个分区，并且不会 shuffle；如果传入的参数比现在的分区数量多，则不会有任何效果，如果要添加分区数量可以使用 repartition 或者使用 coalesce 时 shuffle 参数传入 false。 原理主要原理就是将多个 partition 划分成一个个 partitionGroup，例如前面的例子，有 1000 个分区，需要减少至 100 个分区，那么就会创建 100 个 partitionGroup，每个 partitionGroup 都有 10 个 partition，相当于将 1000 个分区分成 100 组，每组有 10 个分区，而一个 partitionGroup 则作为 CoalescedRDD 的一个分区。 重点看下 getPartitions 和 compute 方法 这里会使用 DefaultPartitionCoalescer 进行 coalesce，然后封装到 CoalescedRDDPartition 中，这样一个 partitionGroup 就封装成一个 partition 了 再看下 DefaultPartitionCoalescer 的 coalesce 通过 setupGroups 和 throwBalls 两个方法之后，会将 dependencesRDD 尽可能按 preferredLocation 划分好分组，放入 val groupArr = ArrayBuffer[PartitionGroup]() 中，最后调用 DefaultPartitionCoalescer 的 getPartitions 返回 PartitionsGroup 数组 再来看下 compute 方法 compute 中的 Partition 就是一个 PartitionGroup，compute 迭代一个 partition 就是迭代一个 partitionGroup 也就是上游的一组 partition，以此来达到减少分区的作用 总结repartition 算子既可以增加分区数量也可以减少分区数量，但代价是会造成 shuffle，所以如果是减少分区操作可以使用 coalesce 算子。使用 coalesce 算子时，如果 shuffle 参数为 false(默认) 则只能减少分区数量，如果 shuffle 参数为 true 则可以增加或减少分数数，相当于 repartition 算子。 参考文章浪尖说spark的coalesce的利弊及原理 Spark RDD之Partition"},{"title":"Spark 聚合算子","date":"2021-06-14T07:44:09.000Z","url":"/2021/06/14/Spark%20%E8%81%9A%E5%90%88%E7%AE%97%E5%AD%90/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"combineByKey在理解 groupByKey 和 reduceByKey 之前先看看 combineByKey combineBykey 需要三个参数：createCombiner、mergeValue、mergeCombiners createCombiner：当碰到未出现过 key 时需要做的处理 mergeValue：当碰到出现过的 key 时需要做的处理 mergeCombiners：分布式环境下，多台机器的结果如何合并 假设我们有下列一个 kv list，现在需要将 value 按照类别放到不同的 list 中，将 key 为水果的 value 都放到一个单独 list 中，将 key 为动物的 value 放到一个单独的 list 中 调用完 combineByKey 后，会返回一个 (String, List[String]) 的映射关系，我们把 res 打印一下可以得出 再贴一段代码，这段代码只需记住 mapSideCombine 默认值为 true 即可 groupByKey再看看 groupByKey，groupByKey 有一个同胞兄弟 groupBy，groupBy 最终还是调用的 groupByKey，看看 groupByKey 的代码 可以看出 groupBy 也是调用 combineByKeyWithClassTag，只不过 createCombiner、mergeValue、mergeCombiners 不是自定义的。 groupByKey 的 createCombiner 是创建一个 buffer，这个 buffer 采用数组存储 mergeValue 则是将值加入数组 mergeCombiners 就将两个数组合并 注意 groupByKey 调用 combineByKeyWithClassTag 时传入的 mapSideCombine 为 false 表示不进行预聚合，即 mergeValue 不是发生在 shuffle 前 图片来自： reduceByKey再看看 reduceByKey 的实现，reduceByKey 也有一个兄弟叫 reduce，不过 reduce 没有调用 reduceByKey，而是使用的 reduceLeft。直接上 reduceByKey 的代码 reduceByKey 最终也是调用 combineByKeyWithClassTag，只不过 mergeValue 和 mergeCombiners 都使用我们自定义的一个函数，不过需要注意这个函数 func: (V, V) =&gt; V，这个函数要求输入的两个参数类型必须一致且返回的类型和输入类型一致，所以一般将输入的两个参数进行聚合再返回。所以聚合后我们得到的是 k -&gt; v 。所以在分布式环境下进行 mergeCombiners 时可以使用和 mergeValue 同一个函数即可。 图片来自： 从上面图中可以看出，groupByKey 在 shuffle 前是不进行预聚合的，而 reduceBykey 则是将 key 相同的数据聚合成一个，图中只是最简单的场景，生产环境中场景更加复杂，数据量大得多，所以在数据量相同的情况，groupByKey 的性能必然要比 reduceByKey 更差。 aggregateByKey先看代码 aggregateByKey 也有三个参数： zeroValue ：‘零值’，聚合之前的初始值，必须是可变的，每一个分区中的每一个 key 都有一个初始值 seqOp：将 V 聚合进 U 的操作，作用于同一个分区里 combOp：将两个 U 合并的操作，作用于不同分区之间 aggregateByKey 和 combineByKey 非常相似，重点关注 combineByKeyWithClassTag[U]((v: V) =&gt; cleanedSeqOp(createZero(), v), cleanedSeqOp, combOp, partitioner) 发现 createCombiner 和 mergeValue 使用的是统一个函数，只不过 createCombiner 时传入了零值。所以当 createCombiner 和 mergeValue 操作一样时优先使用 aggregateByKey foldByKey show code 这个 foldByKey 相较于 aggregateByKey combine 的三个参数都使用一个函数，即 crateCombiner、mergeValue、mergeCombiners 都相同，只不过 crateCombiner 使用了零值 进阶版 combineByKey 这两个进阶版的 combineByKey 比普通版的 combineByKey 分别多了可以指定分区数的 numPartitions 参数和指定分区器的 Partitioner 参数，其它都一致。如果不指定分区器，则使用 self 即调用者的分区器类型 groupByKey、reduceByKey 和 aggregateByKey 的进阶版也一样，都只是增加了指定分区数和分区器的参数。 countByKeycountByKey 顾名思义，能根据 key 计数，看看源码 countByKey 使用的就是 reduceByKey countApproxDistinctByKey统计每个 key 下去重后的近似值，可以根据 relativeSD 参数设置相对精度，默认值是 0.05，但必须大于 0.000017。countApproxDistinctByKey 底层采用 HyperLogLog 统计 总结 combineByKey、groupByKey、reduceBykey、aggregateByKey、foldByKey 这几个算子都是基于 combineByKeyWithClassTag，而 combineByKeyWithClassTag 的核心参数就三个，分别是 crateCombiner、mergeValue、mergeCombiners。 五个算子的主要区别：combinerByKey 和 aggregateByKey 输入输出类型可以不一致，而 reduceByKey 和 foldByKey 要求输入和输出参数一致，groupByKey 就不是聚合算子，只能算分组算子，且性能最差，因为会 shuffle 所有值。 当 crateCombiner 和 mergeValue 是一样的操作时可以选用 aggregateByKey，而 createCombiner、mergeValue 和 mergeCombiners 都一样时，可以选用 foldByKey，根据需要各取所需即可。 参考文章 结合Spark源码分析, combineByKey, aggregateByKey, foldByKey, reduceByKey Avoid GroupByKey"},{"title":"MySQL 常用命令","date":"2021-04-19T16:00:00.000Z","url":"/2021/04/20/MySQL-command/","tags":[["MySQL","/tags/MySQL/"]],"categories":[[" ",""]],"content":" 时间函数 更多时间函数参考 MySQL：日期函数、时间函数总结"},{"title":"Spark DataFrame 自增 ID","date":"2021-04-19T04:42:25.000Z","url":"/2021/04/19/Spark-increment-id/","tags":[["Spark","/tags/Spark/"],["DataFrame","/tags/DataFrame/"]],"categories":[[" ",""]],"content":"Spark 添加一列自增 ID 有两种方式，一种是通过 Spark SQL 的窗口函数，另一种是通过 RDD 的 zipWithIndex 算子。 案例： 输出： 窗口函数方式： 输出： zipWithIndex 算子方式： 输出： 上面 .map&#123;case(x, y) =&gt; (x, y + 1)&#125; 的作用是让 id 初始值为 1"},{"title":"Spark on YARN","date":"2021-04-18T08:30:09.000Z","url":"/2021/04/18/Spark-On-YARN/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"],["YARN","/tags/YARN/"]],"categories":[[" ",""]],"content":" YARN YARN(Yet Another Resource Negotiator) 是 Hadoop 2.x 新增的资源调度器。首先看下 Hadoop 1.x 是怎么调度的 Hadoop 1.x 的运行流程： client 提交任务给 Job Tracker Job Tracker 接收任务，并根据 Job 的参数向 NameNode 请求包含这些文件的 DataNode 节点列表 Job Tracker 确定执行计划：确认 Map、Reduce 的 Task 数量，将这些 Task 分配到离数据块最近的节点上执行 随着发展部署任务越来越多，虽然 Job Tracker 可以部署多个，但是只有一个 Job Tracker 处于 active 状态，其它的 Job Tracker 都是 standby 并不能接收任务，所以 Job Tracker 变成了 Hadoop 的瓶颈。 那么 YARN 是怎么解决这个问题的呢？YARN 采用 Master/Slave 的结构，采用双层调度架构，第一层是 ResourceManager 和 NodeManager : ResourceManager 是 Master 节点，相当于 Job Tracker，有 Scheduler 和 Application Manager 两个组件，分别用于资源调度和应用管理；NodeManager 是 Slave 节点，可以部署在独立的机器上，用于管理及其上的资源。 第二层是 NodeManager 和 Container ，NodeManager 将 CPU 内存等资源抽象成一个个的 Container 并管理它们的生命周期。 这种架构的好处： Scheduler 由原来管理的 CPU 等资源变成了管理 Container 粒度变粗了，降低了负载。 Application Manager 只需要管理 App Master 不需要管理任务调度的完整信息，也降低了负载。 下图为 YARN 的架构图 组件说明： ResourceManager： 定时调度器(Scheduler)：从本质上来说，定时调度器就是一种策略，或者说一种算法。当 Client 提交一个任务的时候，它会根据所需要的资源以及当前集群的资源状况进行分配。注意，它只负责向应用程序分配资源，并不做监控以及应用程序的状态跟踪。 应用管理器(ApplicationManager)：同样，听名字就能大概知道它是干嘛的。应用管理器就是负责管理 Client 用户提交的应用。上面不是说到定时调度器（Scheduler）不对用户提交的程序监控嘛，其实啊，监控应用的工作正是由应用管理器（ApplicationManager）完成的。 NodeManager: Container：容器（Container）这个东西是 Yarn 对资源做的一层抽象。就像我们平时开发过程中，经常需要对底层一些东西进行封装，只提供给上层一个调用接口一样，Yarn 对资源的管理也是用到了这种思想。 需要注意两点： 容器由 NodeManager 启动和管理，并被它所监控。 容器被 ResourceManager 进行调度。 ApplicationMaster 每当 Client 提交一个 Application 时候，就会新建一个 ApplicationMaster 。由这个 ApplicationMaster 去与 ResourceManager 申请容器资源，获得资源后会将要运行的程序发送到容器上启动，然后进行分布式计算。这个 ApplicationMaster 可能运行在某个 Container 中，也可能运行在 Clinet 中，由不同的部署方式决定。 YARN 的部署流程： Client 向 ResourceManager 提交一个作业。 ResourceManager 向 NodeManager 请求一个 Container 并在这个 Container 中运行 ApplicationMaster ApplicationMaster 向 ResourceManager 注册，注册之后，客户端就可以查询 ResourceManager 获得自己 ApplicationMaster 的详情以及直接和 ApplicationMaster 交互； ApplicationMaster 启动后将作业拆分一个个的 Task，然后向 ResourceManager 请求 Container 资源用于运行 Task，并定时向 ResourceManager 发送心跳 ApplicationMaster 请求到资源后，ApplicationMaster 会跟对应的 NodeManager 通信，并将 Task 分发到对应的 NodeManager 中的 Container 中运行。 ApplicationMaster 定时向 ResourceManager 发送心跳，汇报作业运行情况。程序运行完成后再向 ResourceManager 注销释放资源。 Spark On YARN 首先看下 Spark 资源管理架构图： 组件说明： ClusterManager ：也就是 Master，是 Spark 的主控节点，可以部署多个 Master，但是只有一个是 active 状态。 Work：是 Spark 的工作节点，向 Master 汇报资源、Executor 的执行状态，由 Master 控制 Worker 的启动。 Driver：是应用程序的驱动程序，每个应用包含多个小任务，Driver 负责推动这些小任务执行。 Executor：是 Spark 的工作进程，由 Worker 管理，负责具体任务的执行。 Spark 的架构与 YARN 的架构非常像，简单看下角色的对比，Master 与 ResourceManager 相对应，Worker 和 NodeManager 对应，Driver 和 ApplicationMaster 对应，Executor 和 Container 相对应。 Spark 的部署模式： Local 模式：部署在同一个进程中，只有 Driver 角色，接受任务后创建 Driver 负责应用的调度执行，不涉及 Master 和 Worker； Local- Cluster 模式：部署在同一个进程上，存在 Master 和 Worker 角色，它们作为独立线程存在于这个进程内； Standalone：Spark 真正的集群模式，在这个模式下 Master 和 Worker 是独立的进程； 第三方部署模式：构建于 YARN 或者 Mesos 上，由第三方负责资源管理。 Spark On YARN-Cluster 部署流程： Client 向 ResourceManager 提交任务 ResourceManager 接收任务并找到一个 Container 创建 ApplicationMaster，此时 ApplicationMaster 上运行的是 Spark Driver ApplicationMaster 向 ResourceManager 申请 Container 并启动 Spark Driver 在 Container 上启动 Spark Executor，并调度 Spark Task 在 Spark Executor 上运行 等作业执行完后向 ResourceManager 注销释放资源 可以看出这个执行流程和 Yarn 对一个任务的处理过程几乎一致，不同的是在 Spark on Yarn 的 Job 处理过程中 App Master、Container 是交由 Spark 相对应的角色去处理的。 Spark on YARN 还有一种部署方式：Spark On YARN-Client，与 Spark On YARN-Cluster 的区别就是 Spark on Yarn-Client 的客户端在提交完任务之后不会将 Spark Driver 托管给 Yarn，而是在客户端运行。App Master 申请完 Container 之后同样也是由 Spark Driver 去启动 Spark Executor，执行任务。 参考资料 深入浅出 Hadoop YARN Spark on Yarn | Spark，从入门到精通 "},{"title":"JAVA 并发 —— 内存模型","date":"2020-09-20T07:36:09.000Z","url":"/2020/09/20/Java-memory-model/","tags":[["JAVA","/tags/JAVA/"],["并发","/tags/%E5%B9%B6%E5%8F%91/"]],"categories":[[" ",""]],"content":"并发编程模型的两个关键问题线程之间是如何通信和线程之间是如何同步。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。 其中共享内存是指，线程之间通过读-写内存中的公共状态进行隐式通信。而消息传递并发模型中，线程之间没有公共状态，线程之间必须通过发送消息来显示进行通信。 同步是指程序中用于控制不同线程间操作发生相对顺序的机制，在共享内存并发模型中，同步是显示进行的，程序必须显示指定某个方法或者某段代码需要在线程之间互斥执行。而在消息传递并发模型中，由于消息的发送必须在消息的接收之前，所以是隐式进行的。 Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。 Java 内存模型的抽象结构Java 中的实例域、静态域和数组元素都存储在堆内存中，堆内存是线程共享的。而局部变量、方法定义参数和异常处理参数不会在线程间共享。 Java 线程之间的通信由 Java 内存模型（JMM）控制，JMM 决定一个线程对共享变量的写入何时对另一个线程可见。如下图 JMM 抽象结构示意图。 上图中本地内存是一个抽象的概念，并不真实的存在，它包括缓存、写缓冲区、寄存器以及其它硬件和编译器优化。 如上图，如果线程A 要和 线程B 进行通信： 线程A 把本地内存A中更新过的共享变量刷新到主内存中。 线程B 到主内存中读取线程A之前更新过的共享变量 这两个步骤实质上是线程A在向线程B发送消息，而且这个通信过程必须要经过主内存。JMM通过控制主内存与每个线程的本地内存之间的交互，来为Java程序员提供内存可见性保证。 从源代码到指令序列的重排序在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。分为以下3种： 编译器优化的重排序。编译器不改变单线程程序语义前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是乱序执行。 从 Java 源代码到最终指向的指令序列，分别经历上面的三种重排序。 上述1 属于编译器重排序，2、3属于处理器重排序。这些重排序可能会导致多线程程序出现内存可见性问题。对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM的处理器重排序规则会要求Java编译器在生成指令序列时，插入特定类型的内存屏障Memory Barriers，Intel称之为Memory Fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序。 指令重排序和内存屏障 假设处理器A和处理器B按程序顺序并行执行内存访问，最终可能结果为 x=y=0。具体原因如下图所示： 从内存操作实际发生的顺序来看，直到处理器A执行A3来刷新自己的写缓存区，写操作A1才算真正执行了。虽然处理器A执行内存操作的顺序为：A1→A2，但内存操作实际发生的顺序却是A2→A1。此时，处理器A的内存操作顺序被重排序了（处理器B的情况和处理器A一样 ）。 为了保证内存可见性，Java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。JMM把内存屏障指令分为4类 ，如下图： StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他3个屏障的效果。现代的多处理器大多支持该屏障（其他类型的屏障不一定被所有处理器支持）。执行该屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中（Buffer Fully Flush）。 happens-before 简介在JMM中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须要存在happens-before关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 happens-before 规则： 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读 注意：两个操作之间具有happens-before关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。happens-before的定义很微妙 。 重排序重排序是指编译器和处理器为了优化程序性能而对指令序列进行重新排序的一种手段。 数据依赖性如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分为下列3种类型 编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序 (只针对单线程情况下，多线程处理器和编译器不考虑数据依赖性) as-if-serial 语义as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。 为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序。 如下示例： 上面操作的数据依赖如下： 其中 C 依赖于 A，C 依赖于 B，所以 C 不能被排序到 A 和 B 之前，但是 A 与 B 之间是不存在数据依赖的，所以 A 和 B 的顺序是可以被重排序的。以下是重排序后的结果： 程序顺序规则根据 happens-before 的程序顺序规则，上面的例子存在3个 happens-before 关系 A happens-before B B happens-before C A happens-before C 第3个 happens-before 关系是根据 happens-before 传递性推导出来的。虽然 A happens-before B，但是 B 可能在 A 前面执行。如果A happens-before B，JMM并不要求A一定要在B之前执行。JMM仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前。这里操作A的执行结果不需要对操作B可见；而且重排序操作A和操作B后的执行结果，与操作A和操作B按happens-before顺序执行的结果一致。在这种情况下，JMM会认为这种重排序并不非法（not illegal），JMM允许这种重排序。 重排序对多线程的影响 假设有两个线程A和B，A首先执行writer()方法，随后B线程接着执行reader()方法。线程B在执行操作4时，不一定能看到线程A在操作1对共享变量a的写入 。由于操作1和操作2没有数据依赖关系，编译器和处理器可以对这两个操作重排序；同样，操作3和操作4没有数据依赖关系，编译器和处理器也可以对这两个操作重排序。 如果操作1 和操作 2 重拍序，如下图： 操作1和操作2做了重排序。程序执行时，线程A首先写标记变量flag，随后线程B读这个变量。由于条件判断为真，线程B将读取变量a。此时，变量a还没有被线程A写入，在这里多线程程序的语义被重排序破坏了。 当操作3和操作4重排序时会产生什么效果 ，如下图： 操作3和操作4存在__控制依赖__关系。当代码中存在控制依赖性时，会影响指令序列执行的并行度。为此，编译器和处理器会采用猜测（Speculation）执行来克服控制相关性对并行度的影响。以处理器的猜测执行为例，执行线程B的处理器可以提前读取并计算a*a，然后把计算结果临时保存到一个名为重排序缓冲（Reorder Buffer，ROB）的硬件缓存中。当操作3的条件判断为真时，就把该计算结果写入变量i中。如上图，猜测执行实质上对操作3和4做了重排序。重排序在这里破坏了多线程程序的语义。 在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是as-if-serial语义允许对存在控制依赖的操作做重排序的原因）；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。 顺序一致性顺序一致性内存模型是一个理论参考模型，在设计的时候，处理器的内存模型和编程语言的内存模型都会以顺序一致性内存模型作为参照。 数据竞争与顺序一致性Java内存模型规范对数据竞争的定义如下：在一个线程中写一个变量，在另一个线程读同一个变量，而且写和读没有通过同步来排序。 代码中包含数据竞争时，程序执行结果往往与预测的结果不一致。如果一个多线程程序能正确同步，这个程序将是一个没有数据竞争的程序。 JMM对正确同步的多线程程序的内存一致性做了如下保证。如果程序是正确同步的，程序的执行将具有顺序一致性（Sequentially Consistent）——即程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同。马上我们就会看到，这对于程序员来说是一个极强的保证。这里的同步是指广义上的同步，包括对常用同步原语（synchronized、volatile和final）的正确使用。 顺序一致性内存模型顺序一致性内存模型是一个被计算机科学家理想化了的理论参考模型，它为程序员提供了极强的内存可见性保证。顺序一致性内存模型有两大特性。 一个线程中的所有操作必须按照程序的顺序来执行。 （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 顺序一致性内存模型为程序员提供的视图如下图 在概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个左右摆动的开关可以连接到任意一个线程，同时每一个线程必须按照程序的顺序来执行内存读/写操作。从上面图可以看出，在任意时间点最多只能有一个线程可以连接到内存。 这样把所有线程的所有内存读/写操作串行化。 假设有两个线程A和B并发执行。其中A线程有3个操作，它们在程序中的顺序是：A1→A2→A3。B线程也有3个操作，它们在程序中的顺序是：B1→B2→B3。 假设这两个线程使用监视器锁来正确同步：A线程的3个操作执行后释放监视器锁，随后B线程获取同一个监视器锁。那么程序在顺序一致性模型中的执行效果将如下图示。 再假设这两个线程没有做同步，下面是这个未同步程序在顺序一致性模型中的执行示意图 未同步程序在顺序一致性模型中虽然整体执行顺序是无序的，但所有线程都只能看到一个一致的整体执行顺序。以上图为例，线程A和B看到的执行顺序都是：B1→A1→A2→B2→A3→B3。之所以能得到这个保证是因为顺序一致性内存模型中的每个操作必须立即对任意线程可见。 在JMM中就没有这个保证。未同步程序在JMM中不但整体的执行顺序是无序的，而且所有线程看到的操作执行顺序也可能不一致。比如，当前线程写过的数据缓存在本地内存中，并没有刷新到主内存之前，这个写操作只对当前线程可见；从其他线程角度来看，这个写操作根本就没有执行。只有当前线程把本地内存写过的数据刷新到主内存中后，这个操作对其他线程才可见。这个时候，每个线程看到的执行顺序就不一致了。 同步程序的顺序一致性结果 假设A线程执行writer()方法后，B线程执行reader()方法。这是一个正确同步的多线程程序。根据JMM规范，该程序的执行结果将与该程序在顺序一致性模型中的执行结果相同。 顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM中，临界区内的代码可以重排序（但JMM不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。JMM会在退出临界区和进入临界区这两个关键时间点做一些特别处理，使得线程在这两个时间点具有与顺序一致性模型相同的内存视图。虽然线程A在临界区内做了重排序，但由于监视器互斥执行的特性，这里的线程B根本无法“观察”到线程A在临界区内的重排序。这种重排序既提高了执行效率，又没有改变程序的执行结果。 JMM在具体实现上的基本方针为：在不改变（正确同步的）程序执行结果的前提下，尽可能地为编译器和处理器的优化打开方便之门。 未同步程序的执行特性对于未同步或未正确同步的多线程程序，JMM只提供最小安全性：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值（0，Null，False），JMM保证线程读操作读取到的值不会无中生有（Out Of Thin Air）的冒出来。为了实现最小安全性，JVM在堆上分配对象时，首先会对内存空间进行清零，然后才会在上面分配对象（JVM内部会同步这两个操作）。因此，在已清零的内存空间（Pre-zeroed Memory）分配对象时，域的默认初始化已经完成了 。 JMM不保证未同步程序的执行结果与该程序在顺序一致性模型中的执行结果一致。因为如果想要保证执行结果一致，JMM需要禁止大量的处理器和编译器的优化，这对程序的执行性能会产生很大的影响。而且，未同步程序在顺序一致性模型中，整体是无序的且结果无法预知，所以保证未同步程序在两个模型中执行结果一致也没什么意义。 未同步程序在两个模型中的执行特性有如下几个差异： 顺序一致性模型保证单线程内的操作会按程序的顺序执行，而JMM不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。 顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而JMM不保证所有线程能看到一致的操作执行顺序 JMM不保证对64位的long型和double型变量的写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。 在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（Bus Transaction）。总线事务包括读事务（Read Transaction）和写事务（Write Transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其他的处理器和I/O设备执行内存的读/写。 假设处理器A，B和C同时向总线发起总线事务，这时总线仲裁（Bus Arbitration）会对竞争做出裁决，这里假设总线在仲裁后判定处理器A在竞争中获胜（总线仲裁会确保所有处理器都能公平的访问内存）。此时处理器A继续它的总线事务，而其他两个处理器则要等待处理器A的总线事务完成后才能再次执行内存访问。假设在处理器A执行总线事务期间（不管这个总线事务是读事务还是写事务），处理器D向总线发起了总线事务，此时处理器D的请求会被总线禁止。 总线的这些工作机制可以把所有处理器对内存的访问以串行化的方式来执行。在任意时间点，最多只能有一个处理器可以访问内存。这个特性确保了单个总线事务之中的内存读/写操作具有原子性。 在一些32位的处理器上，如果要求对64位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，Java语言规范鼓励但不强求JVM对64位的long型变量和double型变量的写操作具有原子性。当JVM在这种处理器上运行时，可能会把一个64位long/double型变量的写操作拆分为两个32位的写操作来执行。这两个32位的写操作可能会被分配到不同的总线事务中执行，此时对这个64位变量的写操作将不具有原子性。 当单个内存操作不具有原子性时，可能会产生意想不到后果。如下图 如上图，假如一个处理器A写一个Long整型变量，64位写操作被分成两个32位的写操作，且分配到不同事务上执行。同时处理器B的64位读操作被分配到同一个事务中执行。那么B只能读到A写了一半的无效值。 从 JDK5 开始只允许把一个64位long/double型变量的写操作拆分为两个32位的写操作来执行，任意的读操作都必须具有原子性（即任意读操作必须要在单个读事务中执行）。"},{"title":"Java 并发 —— Volatile","date":"2020-09-20T07:15:45.000Z","url":"/2020/09/20/volatile/","tags":[["并发","/tags/%E5%B9%B6%E5%8F%91/"],["Java","/tags/Java/"],["volatile","/tags/volatile/"]],"categories":[[" ",""]],"content":"volatile 是轻量级的 synchronized，它在多处理器开发中保证共享变量的可见性，可见性是指当一个线程修改一个共享变量时，另一个线程能够读取到修改后的值。volatile 的执行成本比 sychronized 更低，因为 volatile 不会引起线程的上下文切换。 1.1 volatile 的定义与原理在连接 volatile 的原理之前，先看看实现原理相关的 CPU 术语 volatile 是怎么保证可见性的呢？通过获取 JIT 编译器生成的汇编指令来查看 Java 代码 instance = new Singleton(); // instance 是 volatile 变量 转换为汇编后 有 volatile 修饰的共享变量在进行写操作时会多出第二行汇编代码，第二行代码是一个 Lock 前缀的指令，在多核处理器下会引发： 将当前处理器缓存行的数据写会到系统内存。 这个写会内存操作会使其它处理器缓存了该内存地址的数据无效 为了提高处理速度，处理器会将系统内存的数据读到高速缓存中（L1、L2），但是对高速缓存操作后不知道什么时候写回到内存中，如果对用 volatile 修饰的变量进行写操作后，JVM 会向处理器发送一条 Lock 前缀的指令，这时就会将修改后的变量写回到内存中，但是，即使将变量写回后，在多处理器的情况下，其它处理器缓存的还是旧的值，所以就有了 缓存一致性协议 。每个处理器通过嗅探总线上传播的数据来检查自己的缓存值是不是过期了，如果发现自己缓存行对应的内存地址进行过修改，那就将自己的缓存行设置为无效，下次进行操作时，再到内存中读到缓存中。 volatile 的两条实现原则 Lock 前缀指令会引起处理器缓存写到内存 以前，在多处理器环境下，Lock# 信号在声言该信号期间，处理器会独占共享内存，对于 Intel486 和奔腾系列处理器，在锁操作时，总是在总线上声言 Lock# 信号。但是，在最近的处理器中 Lock# 信号一般不锁总线，而是锁定缓存。如果需要访问的内存区域已经缓存在处理器内部，则不会声言 Lock# 信号，它会锁定这块内存区域的缓存并写回到处理器内部，并使用缓存一致性机制来保证修改的原子性，称为 缓存锁定 ，缓存一致性会阻止同时修改两个及以上处理器缓存的内存区域数据。 一个处理器将缓存写回到内存导致其它处理器缓存的数据失效 IA-32处理器和Intel 64处理器使用MESI（修改、独占、共享、无效）控制协议去维护内部缓存和其他处理器缓存的一致性。在多核处理器系统中进行操作的时候，IA-32和Intel 64处理器能嗅探其他处理器访问系统内存和它们的内部缓存。处理器使用嗅探技术保证它的内部缓存、系统内存和其他处理器的缓存的数据在总线上保持一致。 volatile 的使用优化在 JDK7 的并发包中新增了一个队列集合类 LinkedTransferQueue，代码如下 它使用内部类来定义头节点（head）和尾节点（tail），这个内部类相对于父类只是将共享变量追加到 64 字节。一个对象引用占 4 个字节，追加 15 个变量（60 字节），再加上父类的 value 变量，一共是64 字节。因为现在主流的处理器的 L1、L2、L3的高速缓存行是64字节宽，不支持充分填充行，所以如果头结点和尾节点都不足64字节时，处理器会将他们读到一个缓存行中，多处理器下会缓存同样的头、尾节点，当一个处理器试图修改头节点时，会将整个缓存行锁定，那么在缓存一致性协议下，其它的处理器不能访问自己缓存的尾节点，严重影响效率。 以下两种情况使用 volatile 变量时不应该追加到64字节 缓存行非64字节宽的处理器。 共享变量不会被频繁写，因为追加字节会导致处理器要读取更多的字节到高速缓存中，会消耗更多的性能。 volatile 的特性理解 volatile 特性的一个好方法就是把对 volatile 变量的单个读写看成是使用同一个锁对这些单个读/写操作做了同步。 如果多个线程分别调用上面程序的3个方法，这个程序的语义和下面的程序等价 可见性。对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种复合操作不具有原子性 volatile写-读建立的happens-before关系从内存语义的角度来说，volatile的写-读与锁的释放-获取有相同的内存效果：volatile写和锁的释放有相同的内存语义；volatile读与锁的获取有相同的内存语义。 假设线程A执行writer()方法之后，线程B执行reader()方法。根据happens-before规则，这个过程建立的happens-before关系可以分为3类： 根据程序次序规则，1 happens-before 2;3 happens-before 4 根据volatile规则，2 happens-before 3 根据happens-before的传递性规则，1 happens-before 4 volatile写-读的内存语义当写一个 volatile 变量时，JMM 会把该线程对应的本地内存中的共享变量值刷新到主内存。 当读一个volatile 变量时，JMM 会把该线程对应的本地内存置为无效，线程接下来将从主内存中读取共享变量。 volatile内存语义的实现下表是JMM针对编译器制定的volatile重排序规则表 当第一个操作为普通变量的读或写时，如果第二个操作为volatile写，则编译器不能重排序这两个操作，可以保证在volatile写之前，其前面的所有普通写操作已经对任意处理器可见了。 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前 当第一个操作是volatile写，第二个操作是volatile读时，不能重排序 为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能。为此，JMM采取保守策略 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的后面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 下面是 volatile 写插入内存屏障后生成的指令序列示意图 StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作已经对任意处理器可见了。这是因为StoreStore屏障将保障上面所有的普通写在volatile写之前刷新到主内存。 这里比较有意思的是，volatile写后面的StoreLoad屏障。此屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。因为编译器常常无法准确判断在一个volatile写的后面是否需要插入一个StoreLoad屏障（比如，一个volatile写之后方法立即return）。为了保证能正确实现volatile的内存语义，JMM在采取了保守策略：__在每个volatile写的后面，或者在每个volatile读的前面插入一个StoreLoad屏障__。从整体执行效率的角度考虑，JMM最终选择了在每个volatile写的后面插入一个StoreLoad屏障。因为volatile写-读内存语义的常见使用模式是：一个写线程写volatile变量，多个读线程读同一个volatile变量。当读线程的数量大大超过写线程时，选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率的提升。从这里可以看到JMM在实现上的一个特点：首先确保正确性，然后再去追求执行效率。 下面是 volatile 读插入内存屏障后生成的指令序列示意图 在实际执行时，只要不改变volatile写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。例如下面这个例子 针对readAndWrite()方法，编译器在生成字节码时可以做如下的优化 注意，最后的StoreLoad屏障不能省略。因为第二个volatile写之后，方法立即return。此时编译器可能无法准确断定后面是否会有volatile读或写，为了安全起见，编译器通常会在这里插入一个StoreLoad屏障。 "},{"title":"Git 基本使用","date":"2020-09-14T11:44:09.000Z","url":"/2020/09/14/Git/","tags":[["git","/tags/git/"],["tool","/tags/tool/"]],"categories":[[" ",""]],"content":"主要参考廖雪峰的官方网站-Git教程，删减上下文信息，把一些命令提取出来进行了简单的解释 基本操作 添加文件到仓库 查看工作区状态 版本回退 工作区和版本库 撤销修改 小结 场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- file。 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD &lt;file&gt;，就回到了场景1，第二步按场景1操作。 场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，参考[版本回退]，不过前提是没有推送到远程库。 删除文件 先手动删除文件，然后使用git rm 和git add效果是一样的 远程仓库 添加远程库 克隆远程仓库 分支管理 创建与合并分支 解决冲突 解决冲突 分支管理策略 上面讲到了合并分支的 Fast forward 方式，在这种模式下，删除分支后，会丢掉分支信息，如果强制禁用 Fast forward 模式，git 在 merge 时会生成一个新的 commit，就能保留分支信息 好的分支管理策略应该像下图一样： bug 分支 当 master 分支出现 bug 需要紧急修复时，需要创建新的 bug 分支进行修复，而自己的工作又只做到一半需要保存时，先把工作现场 修复 master 分支后发现当前工作的分支也有这个 bug，现在只需用 删除没有合并的分支 如果有一个实验性质没有合并的分支需要删除需要 多人协作 与远程分支建立联系 一般多人协作的模式如下： 首先，可以试图用git push origin 推送自己的修改； 如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并； 如果合并有冲突，则解决冲突，并在本地提交； 没有冲突或者解决掉冲突后，再用git push origin 推送就能成功！ 如果git pull提示no tracking information，则说明本地分支和远程分支的链接关系没有创建，用命令git branch –set-upstream-to origin/。 Rebase 理想状态下希望提交记录是一条直线 可以通过 rebase 实现，一般把 rebase 叫变基，概念比较抽象，用下面几张图来解释a 初始情况： 使用 git rebase master 后，bugFix 分支上的工作在 master 的最顶端 然后切换到 master 分支，进行 git rebase bugFix 这里扩展一下 git pull 和 git pull –rebase 的区别 参考：简单对比git pull 和 git pull –rebase 的使用 同时还有一个经常使用的 git rebase -i ，这个命令可以选取你连续提交的多次进行合并。 标签管理"},{"title":"正则表达式-基础","date":"2020-09-14T11:44:09.000Z","url":"/2020/09/14/regular-expressions/","tags":[["regular","/tags/regular/"],["正则表达式","/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"]],"categories":[[" ",""]],"content":" 开头结尾^a 匹配的字符串以 a 开头（括号里除外） a$ 匹配字符串已 a 结尾 字符 [abc]，表示匹配中括号中的任意字符 [a-zA-Z0-9]，范围表示 [^a-z]，取反，表示不等于 a-z 中任意字符 .，表示任意字符（\\n 除外） \\s，表示空白字符，等同于 [\\r\\n\\t\\f\\v ] \\S，表示除了空白字符，即 [^\\r\\n\\t\\f\\v ] \\w，匹配字母数字下划线，等同于 [a-zA-Z0-9_] \\W，\\w 取反 \\d，匹配任意一个数字，即 [0-9] \\D，\\d 取反 上面几个转意匹配的，只要大小字母同时出现就可以匹配任意字符即 [\\s\\S] 就可以匹配任意字符。以上都是表示匹配单个字符，下面是匹配多个字符： a?，匹配 0 个或 1 个 a a*，匹配 0 个到无限个 a a+，匹配 1 个到无限个 a a&#123;m&#125;，匹配连续出现 m 次的 a a&#123;m,n&#125;，匹配连续出现 m - n 的 a a&#123;m,&#125;，匹配连续出现 m - 无穷的 a "},{"title":"JAVA8 新特性1 —— Lambda 表达式","date":"2020-09-14T09:15:45.000Z","url":"/2020/09/14/JAVA8-Lambda/","tags":[["JAVA","/tags/JAVA/"],["Lambda","/tags/Lambda/"],["JAVA8","/tags/JAVA8/"]],"categories":[[" ",""]],"content":"JAVA8的新特性核心是Lambda表达式和Steam API 一、Lambda表达式：1、语法：Java8中引入了一个新的操作符“-&gt;”，称为箭头操作符或Lambda操作符，箭头操作符将Lambda表达式拆分成两部分：左侧：Lambda表达式的参数列表右侧：Lambda表达式中所执行的功能，也称Lambda体 语法格式一：无参数，无返回值 语法格式二：有一个参数且无返回值 语法格式三：有两个及以上的参数，有返回值，并且Lambda体中有多条语句 语法格式四：如果Lambda体只有一条语句，大括号和 return 可以省略不写 注：Lambda 表达式的参数列表的数据类型可以不用写，JVM可以通过上下文可以自动推断数据类型。 2、Lambda 表达式需要“函数式接口”支持函数式接口：接口中只有一个抽象方法的接口，称为函数式接口。可以使用注解@FunctionalInterface 修饰检查是否为函数式接口用一个例子来说明： Lambda使用：①：声明一个函数式接口，接口中声明一个抽象方法。②：类中编写方法使用接口作为参数。③: 在调用②中方法时，接口参数部分使用 Lambda 表达式。 JAVA8 提供以下四大核心函数式接口 更多函数式接口可在java8官方文档中查看 Lambda 方法引用若Lambda体中的内容有方法已经实现了，可以用“方法引用”注意：lambda体中调用方法的参数列表与返回类型必须一致 1、类::静态方法名 2、对象::实例方法名 3、类::实例方法名若Lambda 参数列表的第一个参数是实例方法的调用者，第个参数是实例方法的参数是，可以使用ClassName::method 构造器引用ClassName :: new注意：需要调用的构造器的参数列表要与函数式接口中的抽象方法列表必须一致！ 数组引用格式：Type[]::new "},{"title":"JAVA NIO","date":"2020-09-13T11:44:09.000Z","url":"/2020/09/13/Java%20NIO/","tags":[["JAVA","/tags/JAVA/"],["NIO","/tags/NIO/"],["并发","/tags/%E5%B9%B6%E5%8F%91/"]],"categories":[[" ",""]],"content":"直接内存，间接内存&#8195;java.nio 从 Java 1.4开始引入，可以叫New I/O，也可叫Non-Blocking I/O。java.nio 有三个核心概念Selector、Channel、Buffer，在java.nio中我们是面向块（block）或者缓冲区（buffer）编程，而不是像 java.io 中的面向流编程。buffer 是内存中的一块区域，底层的实现是数组，所有数据的读或者写都是通过 buffer 来实现。 &#8195;对于Java的8中基本数据类型都有（除了 Boolean）对应的 Buffer 类型，如 ByteBuffer、CharBuffer、IntBuffer 等&#8195;Channel 是指可以写入或者读取数据的对象，类似于 java.io 中的 Stream，不过 Channel 是双向的，可以进行读写。但是所有的读写操作都是通过 Buffer 进行，不会直接通过 Channel 读写数据。 Buffer&#8195;Buffer 中有几个重要的属性 mark、position、limit、capacity，其中 0 &lt;= mark &lt;= position &lt;= limit &lt;= capacity。&#8195;capacity 表示缓冲区 Buffer 的容量，不能为负数。limit 为缓冲区的限制，不能为负，限制代表缓冲区中第一个不能读取或者写入元素的索引（下标）。&#8195;position 代表下一个要读取或者写入元素的索引（下标），不能为负。&#8195;mark 表示缓冲区的标记，标记的作用是调用 reset() 方法时，会将 position 位置重置到 mark 位置，标记不是必须的，而且标记不能大于 position，如果定义了 mark ，再将 position 或 limit 重置到比 mark 小的位置时会丢弃 mark，将 mark 置为 -1。如果未定义 mark 在调用 reset() 方法时会抛出 InvalidMarkException 异常。总结： 1 ）缓冲区的 capacity 不能为负数，缓冲区的 limit 不能为负数，缓冲区的 position 不能为负数 。2) position 不能大于其 limit 。3) limit 不能大于其 capacity 。4 ）如果定义了 mark ，则在将 position 或 limit 调整为小于该 mark 的值时，该 mark 被丢弃 。5 ）如果未定义 mark ，那么调用 reset（） 方法将导致抛出 InvalidMarkException 异常 。6 ）如果 position 大于新的 limit ，则 position 的值就是新 limit 的值 。7 ）当 limit 和 position 值一样时，在指定的 position 写入数据时会 出现异常，因为此位置是被限制的 。 flip() 方法例子： 上述例子中，将 niotest1.txt 读入 buffer 后进行了一次 flip 操作，下面是 flip 方法的源码。flip 操作将 limit 设置为当前的 position，下次读取操作时就不会超过赋值的界限，保证读取的数据都是有效的。然后 position 设置为 0，下次读取时能从下标 0 开始读，mark 设置为 -1 clear() 方法&#8195;clear 方法只是将 limit 设置为 capacity，position 设置为 0，并没有将数据删除，而只是将 buffer 数组设置为初始状态，下次写操作时直接覆盖，而读操作可以把原来的数据读出来。下面是 clear 方法的源码 相对位置和绝对位置ByteBuffer 类型化 put 和 get方法就是，将其他类型 put 进 ByteBuffer，但是，put 什么类型，get 就是什么类型，顺序不能变。 共享底层数组 slice 共享相同的数组直接缓冲和零拷贝 DirectBuffer 内存映射文件 MappedByteBuffer将文件的全部或者一部分映射到堆外内存中，Java即可以直接操作内存，而不用操作文件，减少I/O操作，提升操作效率 关于 Buffer 的 Scattering（分散）和 Gathering（收集）Scattering 是指在使用 Channel 进行读取的时候，如果我们传入的是一个 buffer 数组，那么会将第一个 buffer 读满后再读入第二个 buffer 依次进行。Gathering 是指写出的时候传入一个 buffer 数组，会将第一个 buffer 全部写出，再将第二个 buffer 全部写出，依次进行。 unicode 是编码方式utf 是存储方式utf-8 是unicode的实现方式"},{"title":"JAVA8 新特性2 —— Stream API","date":"2020-09-13T09:25:45.000Z","url":"/2020/09/13/JAVA8-Stream%20API/","tags":[["JAVA","/tags/JAVA/"],["JAVA8","/tags/JAVA8/"],["Stream API","/tags/Stream-API/"]],"categories":[[" ",""]],"content":"Steam API (java.util.stream.*) 三个步骤①：将数据源转换成流②：一系列中间操作③：产生一个新流（不改变源数据） 流（Stream）是什么？是数据渠道，用于操作数据源（集合、数组等）所生成的元素序列。集合讲的是数据，流讲的是计算！ 注意：①：Stream 自己不会存储元素。②：Stream 不会改变源对象。会返回一个处理后的新Stream。③：Stream 操作是延迟执行的。要等到需要结果的时候才执行 一、创建 Stream 1、可以通过Collection 系列集合提供的 stream() (串形流) 或 parallelStream() (并行流)获取流 2、通过Arrays 中的静态方法 stream() 获取数组流 3、通过 Stream 类中的静态方法 of() 4、无限流 此时的流是没有任何效果的，只是被创建出来了，要通过中间操作或者终止操作才有效果 二、中间操作 中间操作，如果没有终止操作是不会有任何的执行 1、筛选与切片 filter：接收 Lambda，从流中排除某些元素 limit：截断流，使元素不超过给定数量。 skip(n)：跳过元素，返回一个扔掉前 n 个元素的流，若流中不足 n 个，返回空流，与 limit 互补 distinct：筛选，通过流所生成的 hashCode() 和 equals() 去除重复元素 终止操作：一次性执行全部内容，叫“惰性求值”或“延时加载“ 外部迭代 2、映射 map：接收 Lambda，将元素转换成其他形式或提取信息。接收一个函数作为参数，该函数会被应用每一个元素上，并将其映射成一个新的元素。 flatMap：接收一个函数作为参数，将流中的每个值都换成另一流，然后把所有流连接成一个流 map 案例 输出 flatMap 输出结果 map 和 flatMap 有点像 List 的 add 和 addAll 3、排序sorted：自然排序sorted(Comparator com)：订制排序 三、终止操作 1、查找与匹配allMatch：检查是否匹配所以元素anyMatch：检查是否至少匹配一个元素noneMatch：检查是否没有匹配所有元素findFirst：返回第一个元素findAny：返回当前流的任意元素count：返回流中元素的总个数max：返回流中最大值min：返回流中最小值 2、归约reduce(T identity, BinaryOperator)：将流中元素反复结合起来，得到一个值 3、收集collect：将流转换成其他形式，接收一个Collector接口实现，用于给Stream中元素做汇总的方法。 "},{"title":"ArrayList 源码解析","date":"2020-09-13T07:44:09.000Z","url":"/2020/09/13/ArrayList/","tags":[["JAVA","/tags/JAVA/"],["ArrayList","/tags/ArrayList/"],["集合","/tags/%E9%9B%86%E5%90%88/"],["源码","/tags/%E6%BA%90%E7%A0%81/"]],"categories":[[" ",""]],"content":"首先存储的数据结构为：transient Object[] elementData; 构造方法构造方法有三种： 无参构造方法：创建一个默认容量的空数组，但是这里用的是 目的是与 区分，知道第一次添加元素该扩容多少，挖坑，后续填 带初始容量的构造方法：在 new ArrayList 的时候指定一个整数参数，这个参数为初始容量大小，这里会做一个判断，分三种情况 参数正常，非负且大于0，创建一个大小为传入值的数组 参数为0，使用 EMPTY_ELEMENTDATA 异常 初始参数为集合，构造一个包含指定集合元素的列表，不过传入的列表的对象必须是 Collection&lt;? extends E&gt; 如果传入的集合没问题则创建一个元素为传入集合的List 如果传入的集合大小为0，则使用 EMPTY_ELEMENTDATA 添加元素添加元素的方法主要有四种 add(E e) 在添加元素之前要确保容量够，会根据一个原有 size + 1 生成的minCapacity 去进行比较，如果为空时，且是之前无参构造方法创建的对象（DEFAULTCAPACITY_EMPTY_ELEMENTDATA），会扩容到默认初始值为 10，如果是之前有参构造函数创建的方法，就判断 minCapacity 是否大于原来数组的长度，如果比原来数组长度小，即添加的元素要数组越界了，必须先扩容再添加。 首先扩容是原来容量的 1.5 倍，这里还有一个容量最大问题，如果容量超过了 MAX_ARRAY_SIZE，要么溢出，要么使用 Integer.MAX_VALUE，至于为什么 MAX_ARRAY_SIZE 为 nteger.MAX_VALUE - 8，官方说法是有些虚拟机在数组中保留了一些”header words”，需要给这些“header words”留一些空间。 中间还有一个 newCapacity - minCapacity &lt; 0 的判断，以我的理解应该是 oldCapacity 已经非常大了，再增长 1.5 倍就会溢出了，所以如果溢出了就增长 1 即可。 最后，添加元素 add(int index, E element) 指定位置插入 步骤与 1 基本一样，只是最后需要从 index开始都往后挪一个位置 再把插入的值放到 index 位置上。 addAll(Collection&lt;? extends E&gt; c) 增加一个集合 步骤与 1 也一样，只是确保容量的时候传的是原来 size + c.length，然后 addAll(int index, Collection&lt;? extends E&gt; c) 指定位置插入集合 步骤与 2 类似，只是将往后移动一个换成往后移动一段 删除元素 remove(int index) 先验证 index 是否合法，然后保存原来的值，最后 然后返回删除的值 remove(Object o) 这个方法是删除对象的，删除 List 中出现的第一个与参数相等的对象，且不返回删除值。这里如果传入 null 则用 == 进行比较，如果非空则用 equals 进行比较，找到相等的后，调用一个私有的不进行 index 检查，不返回值的 fastRemove方法 clear() 对数组每个元素置空，将 size 置0 removeAll(Collection&lt;?&gt; c) 采用覆盖的方式，对原数组进行遍历，判断 c 中是否包含，如果包含就直接覆盖 最后再对未被覆盖且不需要的元素置空，以便 GC retainAll(Collection&lt;?&gt; c) 这个方法与 removeAll 相反，是保留，实现方式与 removeAll 一样，只是 complement = true 查询数据 get(int index) 直接返回下标为 index 的元素 indexOf(Object o) 遍历数组，返回第一个等于 o 的元素的下标，否则返回 -1 lastIndexOf(Object o) 反向遍历数组，返回最后一个等于 o 的元素的下标，否则返回 -1 contains(Object o) 调用 indexOf()，判断返回值是否 &gt;=0 subList(int fromIndex, int toIndex) subList 返回一个子 List，但共用父 List，只是在每个操作时对下标添加一个 offset，特别是 add 操作，每次添加都要移动父 List 的元素，所以效率不高。 "},{"title":"search","date":"2021-11-14T10:36:54.000Z","url":"/search/index.html","categories":[[" ",""]]},{"title":"tags","date":"2021-11-14T10:39:50.000Z","url":"/tags/index.html","categories":[[" ",""]]}]